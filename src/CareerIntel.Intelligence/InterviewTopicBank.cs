namespace CareerIntel.Intelligence;

public sealed record TopicArea(
    string Id,
    string Name,
    string Description,
    IReadOnlyList<StaticInterviewQuestion> Questions,
    IReadOnlyList<string> KeyConcepts,
    IReadOnlyList<LearningResource> Resources);

public sealed record StaticInterviewQuestion(
    string Question,
    string ExpectedAnswer,
    string Difficulty,
    IReadOnlyList<string> Tags);

public sealed record LearningResource(
    string Title,
    string Type,
    string Url);

public static class InterviewTopicBank
{
    public static IReadOnlyList<TopicArea> GetAllTopics() =>
    [
        BuildAlgorithmsAndDataStructures(),
        BuildSystemDesign(),
        BuildDotNetInternals(),
        BuildBackendArchitecture(),
        BuildDatabases(),
        BuildOrmDataAccess(),
        BuildPerformance(),
        BuildConcurrency(),
        BuildCloud(),
        BuildSecurity(),
        BuildTesting(),
        BuildFrontend(),
        BuildDevOps(),
        BuildBehavioral(),
    ];

    public static TopicArea? GetTopic(string id) =>
        GetAllTopics().FirstOrDefault(t => string.Equals(t.Id, id, StringComparison.OrdinalIgnoreCase));

    // ───────────────────────────────────────────────
    // Topic 1: Algorithms & Data Structures
    // ───────────────────────────────────────────────
    private static TopicArea BuildAlgorithmsAndDataStructures() => new(
        Id: "algorithms",
        Name: "Algorithms & Data Structures",
        Description: "Core computer science fundamentals including complexity analysis, common data structures and their .NET implementations, classic algorithm patterns, and problem-solving techniques essential for technical interviews.",
        Questions:
        [
            new(
                Question: "Explain Big-O notation and walk through the time complexity of common operations on a Dictionary<TKey, TValue>.",
                ExpectedAnswer: "Big-O notation describes the upper bound of an algorithm's growth rate as input size increases, ignoring constant factors and lower-order terms. For Dictionary<TKey, TValue> in .NET, which is implemented as a hash table with separate chaining, the average-case complexities are: Add is O(1) amortized because the internal array occasionally resizes (doubling capacity), ContainsKey and TryGetValue are O(1) average-case since we compute the hash code, map it to a bucket index, and typically traverse a very short chain, and Remove is also O(1) average-case. The worst case for lookup degrades to O(n) if all keys produce hash collisions and land in the same bucket, forming a long chain. The amortized O(1) for Add comes from the fact that resizing copies all n entries at a cost of O(n), but this happens only when count exceeds capacity, so spread over n insertions the cost per insertion averages to O(1). In .NET's implementation, the bucket count is always prime to help distribute hash codes more uniformly and reduce clustering.",
                Difficulty: "Junior",
                Tags: ["big-o", "dictionary", "hash-table", "amortized-analysis"]),
            new(
                Question: "How does HashSet<T> handle collisions internally in .NET, and what happens during a resize?",
                ExpectedAnswer: "HashSet<T> in .NET uses an array of bucket indices and a parallel array of Entry structs. Each Entry contains the hash code, the value, and a 'next' index that forms a singly-linked chain for collision resolution via separate chaining. When you add an element, .NET computes GetHashCode(), takes the absolute value, then mods by the bucket count to find the bucket index. If that bucket already has entries, the new entry is prepended to the chain by setting its 'next' pointer to the current head. During a resize, which is triggered when the count exceeds the bucket array length, .NET allocates new arrays with a size equal to the next prime number at least double the current capacity. Every existing entry is re-bucketed because the modulus divisor changed. This is an O(n) operation but happens infrequently enough that Add remains O(1) amortized. One important detail: .NET also stores the full hash code in each entry so that during lookup or resize it can compare hash codes before calling Equals(), which is a significant performance optimization because integer comparison is cheaper than a full Equals() call.",
                Difficulty: "Mid",
                Tags: ["hashset", "hash-collisions", "separate-chaining", "resize"]),
            new(
                Question: "Compare and contrast BFS and DFS for tree and graph traversal. When would you choose one over the other?",
                ExpectedAnswer: "Breadth-First Search (BFS) explores nodes level by level using a queue, while Depth-First Search (DFS) explores as deep as possible along each branch before backtracking, using a stack or recursion. BFS guarantees the shortest path in an unweighted graph because it visits all nodes at distance k before any node at distance k+1. DFS is more memory-efficient for deep or wide graphs because it only needs to store the current path on the stack, whereas BFS must store all nodes at the current frontier which can be O(b^d) where b is the branching factor and d is the depth. Choose BFS when you need shortest path in an unweighted graph, when the solution is expected to be close to the root, or when you want to explore neighbors first like in social network friend suggestions. Choose DFS when exploring all possible paths such as in maze solving or generating permutations, when memory is constrained and the graph is very wide, or when performing topological sorting on a DAG. DFS has three common orderings for trees: pre-order (process node, then children), in-order (left, node, right — gives sorted order on a BST), and post-order (children, then node — useful for deletion and expression evaluation). Both have O(V + E) time complexity for adjacency list representations.",
                Difficulty: "Junior",
                Tags: ["bfs", "dfs", "graph-traversal", "tree-traversal"]),
            new(
                Question: "Describe the differences between a BST, AVL tree, and Red-Black tree. Which does SortedDictionary<TKey, TValue> use?",
                ExpectedAnswer: "A Binary Search Tree (BST) maintains the invariant that left children are less than the parent and right children are greater. A plain BST can degenerate to O(n) operations if elements are inserted in sorted order, producing a linear chain. An AVL tree is a self-balancing BST where the height difference between left and right subtrees of any node is at most 1. After each insertion or deletion, rotations restore this balance, guaranteeing O(log n) for all operations. AVL trees are more strictly balanced than Red-Black trees, making lookups faster but insertions and deletions slightly slower due to more rotations. A Red-Black tree is a self-balancing BST where each node is colored red or black, with rules: the root is black, no two consecutive red nodes are allowed, and every path from root to a null leaf has the same number of black nodes. This guarantees the longest path is at most twice the shortest, giving O(log n) operations with fewer rotations on average during mutations compared to AVL. SortedDictionary<TKey, TValue> in .NET uses a Red-Black tree internally. This choice favors balanced insert/delete/lookup performance. SortedSet<T> also uses a Red-Black tree. The practical difference is that AVL is preferred for read-heavy workloads while Red-Black is preferred when insertions and deletions are frequent, which is why most standard library implementations including .NET, Java TreeMap, and C++ std::map use Red-Black trees.",
                Difficulty: "Mid",
                Tags: ["bst", "avl-tree", "red-black-tree", "sorted-dictionary"]),
            new(
                Question: "Explain Dijkstra's algorithm and its limitations. How would you implement it efficiently in C#?",
                ExpectedAnswer: "Dijkstra's algorithm finds the shortest path from a source node to all other nodes in a weighted graph with non-negative edge weights. It maintains a set of visited nodes and a priority queue of tentative distances. Initially, the source distance is 0 and all others are infinity. At each step, we extract the node with the minimum tentative distance, mark it visited, and relax all its neighbors — if the path through the current node offers a shorter distance to a neighbor, we update it. The key limitation is that Dijkstra does not work correctly with negative edge weights because once a node is visited, its distance is considered final, but a negative edge discovered later could provide a shorter path. For negative weights, use Bellman-Ford instead. For efficient implementation in C#, use a PriorityQueue<TElement, TPriority> introduced in .NET 6. Represent the graph as an adjacency list using Dictionary<int, List<(int neighbor, int weight)>>. Initialize a distances array with int.MaxValue, set dist[source] = 0, and enqueue the source with priority 0. On each dequeue, skip if the node is already finalized, otherwise process its neighbors. The time complexity with a binary heap is O((V + E) log V). A practical optimization is to use a visited HashSet and skip already-visited nodes when dequeued rather than trying to decrease-key, since .NET's PriorityQueue does not support decrease-key natively.",
                Difficulty: "Senior",
                Tags: ["dijkstra", "shortest-path", "priority-queue", "graph-algorithms"]),
            new(
                Question: "What is dynamic programming? Walk through the approach for the classic 0/1 Knapsack problem.",
                ExpectedAnswer: "Dynamic programming (DP) is an optimization technique that solves complex problems by breaking them into overlapping subproblems and storing their solutions to avoid redundant computation. It requires two properties: optimal substructure (an optimal solution contains optimal solutions to subproblems) and overlapping subproblems (the same subproblems are solved multiple times). For the 0/1 Knapsack problem, you have n items each with a weight and value, and a knapsack with capacity W. You must maximize total value without exceeding capacity, and each item is either taken or not (no fractions). Define dp[i, w] as the maximum value achievable using items 1..i with capacity w. The recurrence is: if weight[i] > w, then dp[i, w] = dp[i-1, w] (cannot take item i); otherwise dp[i, w] = max(dp[i-1, w], dp[i-1, w - weight[i]] + value[i]). The base case is dp[0, w] = 0 for all w. Build the table bottom-up in O(nW) time and O(nW) space. You can optimize space to O(W) by using a single array and iterating w from W down to weight[i] so you do not overwrite values needed for the current row. To reconstruct which items were selected, trace back from dp[n, W]: if dp[i, w] differs from dp[i-1, w], item i was included. This approach transforms an exponential brute-force O(2^n) into a pseudo-polynomial solution.",
                Difficulty: "Mid",
                Tags: ["dynamic-programming", "knapsack", "memoization", "optimization"]),
            new(
                Question: "Explain the sliding window technique and provide an example of solving 'longest substring without repeating characters'.",
                ExpectedAnswer: "The sliding window technique maintains a window defined by two pointers (left and right) that slide over an array or string. It is useful for problems involving contiguous subarrays or substrings where you need to find an optimal window satisfying a condition. The window expands by moving the right pointer and contracts by moving the left pointer. For the 'longest substring without repeating characters' problem (LeetCode 3), use a HashSet or Dictionary to track characters in the current window. Initialize left = 0 and maxLength = 0. Iterate right from 0 to s.Length - 1. If s[right] is already in the set, remove s[left] from the set and increment left, repeating until the duplicate is removed. Add s[right] to the set and update maxLength = Math.Max(maxLength, right - left + 1). A more optimal approach uses a Dictionary<char, int> mapping characters to their last seen index. When a duplicate is found at position right, jump left directly to Math.Max(left, lastSeen[s[right]] + 1) instead of incrementing one by one. This gives O(n) time and O(min(n, alphabet_size)) space. The key insight is that the window always represents a valid substring without repeats, and we only need to track the maximum window size seen during the traversal.",
                Difficulty: "Mid",
                Tags: ["sliding-window", "two-pointer", "string-algorithms", "leetcode"]),
            new(
                Question: "Describe the two-pointer technique and demonstrate its use for the 'container with most water' problem.",
                ExpectedAnswer: "The two-pointer technique uses two indices that move toward each other or in the same direction to solve problems in O(n) time that might otherwise require O(n^2). It works well when the data has some ordering property or when you can reason about which pointer to move to improve the solution. For 'container with most water' (LeetCode 11), you have an array of heights and want two lines that, together with the x-axis, form a container holding the most water. Initialize left = 0 and right = heights.Length - 1. The area between them is Math.Min(heights[left], heights[right]) * (right - left). The key insight is: always move the pointer pointing to the shorter line inward. Why? The width is decreasing, so the only way to potentially find a larger area is to find a taller line. Moving the taller pointer inward can never increase the area because the area is constrained by the minimum height, and the width is shrinking. After computing the area at each step, update the maximum and move the appropriate pointer. This runs in O(n) time and O(1) space. The correctness proof relies on the fact that by always moving the shorter side inward, we never skip a pair that could be the optimal solution, because any pair we skip includes the shorter line which cannot contribute to a better answer than what we have already recorded.",
                Difficulty: "Mid",
                Tags: ["two-pointer", "greedy", "leetcode", "array-algorithms"]),
            new(
                Question: "How would you approach decomposing an unfamiliar LeetCode-style problem in an interview setting?",
                ExpectedAnswer: "A systematic decomposition approach has several phases. First, clarify the problem: restate it in your own words, confirm input/output types, identify constraints (size of n, value ranges, sorted or unsorted), and ask about edge cases (empty input, single element, negative numbers, duplicates). Second, work through examples: trace through the given examples manually and create your own edge-case examples. Third, identify the pattern: determine if the problem maps to a known pattern — is it a sliding window problem (contiguous subarray optimization), two-pointer (sorted array pair finding), BFS/DFS (tree or graph exploration), dynamic programming (optimal substructure with overlapping subproblems), greedy (local optimum leads to global optimum), binary search (monotonic search space), backtracking (enumerate possibilities with pruning), or divide and conquer (split into independent subproblems). Fourth, start with brute force: articulate the naive O(n^2) or O(2^n) solution to demonstrate understanding, then optimize. Fifth, analyze time and space complexity of your proposed solution before coding. Sixth, code incrementally: write clean code with meaningful variable names, handle edge cases first with early returns, and talk through your logic. Seventh, verify with your examples: dry-run your code on the examples. Eighth, discuss trade-offs: mention alternative approaches and their complexity differences.",
                Difficulty: "Junior",
                Tags: ["problem-solving", "interview-strategy", "decomposition", "leetcode"]),
            new(
                Question: "Compare sorting algorithms: when would you choose QuickSort vs MergeSort vs TimSort? What does Array.Sort use in .NET?",
                ExpectedAnswer: "QuickSort has O(n log n) average-case but O(n^2) worst-case time complexity, is in-place (O(log n) stack space), and is not stable. MergeSort guarantees O(n log n) worst-case, is stable, but requires O(n) extra space. TimSort is a hybrid of MergeSort and InsertionSort that exploits existing partial ordering (natural runs) in data, is stable, and has O(n log n) worst-case with O(n) space. Choose QuickSort when average-case performance matters and space is constrained, and the data is random. Choose MergeSort when you need guaranteed O(n log n) and stability, or when working with linked lists where the O(n) space overhead is eliminated. TimSort excels when data is partially sorted, which is common in real-world datasets. In .NET, Array.Sort uses an introspective sort (IntroSort) which begins with QuickSort, switches to HeapSort when recursion depth exceeds 2 * log2(n) to guarantee O(n log n) worst-case, and falls back to InsertionSort for small partitions (16 elements or fewer) because InsertionSort has lower constant factors for small n. This hybrid approach gives O(n log n) worst-case guarantee while maintaining QuickSort's cache-friendly average-case performance. Note that Array.Sort is not stable in .NET. If you need a stable sort, use OrderBy from LINQ, which uses a stable QuickSort variant, or Enumerable.Order in .NET 7 and later.",
                Difficulty: "Senior",
                Tags: ["sorting", "quicksort", "mergesort", "timsort", "introsort"]),
        ],
        KeyConcepts:
        [
            "Time/Space complexity",
            "Amortized analysis",
            "Hash collisions and separate chaining",
            "BST vs AVL vs Red-Black trees",
            "BFS/DFS traversal",
            "Dijkstra's shortest path",
            "Topological sort",
            "Dynamic programming patterns",
            "Sliding window technique",
            "Two-pointer technique",
        ],
        Resources:
        [
            new("Introduction to Algorithms (CLRS)", "Book", "https://mitpress.mit.edu/9780262046305/introduction-to-algorithms/"),
            new(".NET Source: Dictionary<TKey, TValue>", "Documentation", "https://source.dot.net/#System.Private.CoreLib/Dictionary.cs"),
            new("NeetCode Roadmap", "Practice", "https://neetcode.io/roadmap"),
            new("Big-O Cheat Sheet", "Article", "https://www.bigocheatsheet.com/"),
        ]);


    // ───────────────────────────────────────────────
    // Topic 2: System Design
    // ───────────────────────────────────────────────
    private static TopicArea BuildSystemDesign() => new(
        Id: "system-design",
        Name: "System Design",
        Description: "High-level and low-level system design skills covering distributed systems, scalability patterns, data partitioning strategies, and architecture trade-offs commonly discussed in senior-level interviews.",
        Questions:
        [
            new(
                Question: "Design a URL shortener service like bit.ly. Walk through the high-level architecture and key design decisions.",
                ExpectedAnswer: "A URL shortener maps a short code to a long URL and redirects users. The write path accepts a long URL via a POST API, generates a unique short code, stores the mapping in a database, and returns the short URL. The read path receives a GET request with the short code, looks up the long URL, and returns a 301 (permanent) or 302 (temporary) redirect. For code generation, use a base-62 encoding of an auto-incrementing ID from a distributed ID generator like Snowflake, or use a hash of the URL (MD5/SHA256) truncated to 7 characters with collision handling. With 7 base-62 characters you get 62^7 = 3.5 trillion unique codes. The storage layer needs a key-value-like access pattern: given a short code, return the long URL. A relational database like PostgreSQL works with an index on the short code column, or you can use a NoSQL store like DynamoDB for its fast key-value lookups. For scale, add a caching layer (Redis) in front of the database since reads vastly outnumber writes — cache the top accessed URLs with an LRU policy. A CDN or edge cache can further reduce latency. For analytics (click counts, referrers), write events asynchronously to a message queue like Kafka and process with a stream consumer. Rate-limit the creation endpoint to prevent abuse. For high availability, deploy the API behind a load balancer across multiple availability zones with database replicas for read scaling.",
                Difficulty: "Senior",
                Tags: ["url-shortener", "system-design", "distributed-id", "caching"]),
            new(
                Question: "How would you design a real-time chat system supporting one-on-one and group messaging?",
                ExpectedAnswer: "The system needs real-time message delivery, persistent storage, presence tracking, and offline message queuing. Clients connect via WebSockets to a Chat Service for real-time bidirectional communication. When User A sends a message to User B, the Chat Service looks up B's connected server via a Presence Service backed by Redis. If B is on the same server, deliver directly. If on another server, route through a message broker like Redis Pub/Sub or Kafka. If B is offline, store the message in an undelivered messages queue and push a notification via a Push Notification Service (APNs/FCM). For persistent storage, use a database optimized for time-series writes — Cassandra is a good fit because its partition key can be the chat_id and clustering key the timestamp, giving efficient range queries for chat history. For group chats, fan out the message to all group members. Small groups (under 100) can use write-time fan-out where the message is sent to each member's queue. Large groups or channels should use read-time fan-out where the message is stored once and members pull it. The Presence Service tracks online status using heartbeats — each client sends a heartbeat every 30 seconds; if missed, the user is marked offline. Use a Session Service to map user IDs to WebSocket server IDs. For message ordering, use server-side timestamps with Lamport-style logical clocks to handle clock skew. Support message delivery receipts (sent, delivered, read) by having the client acknowledge receipt.",
                Difficulty: "Senior",
                Tags: ["chat-system", "websockets", "real-time", "message-broker"]),
            new(
                Question: "Design a notification service that handles email, SMS, and push notifications at scale.",
                ExpectedAnswer: "The Notification Service receives notification requests via an API or event stream and routes them to the appropriate delivery channel. The architecture has three layers: ingestion, processing, and delivery. The ingestion layer exposes a REST API and also consumes events from a message broker (Kafka). Each request includes the recipient, notification type, channel preference, template ID, and parameters. The processing layer validates the request, checks user preferences (opt-in/opt-out stored in a Preferences database), applies rate limiting per user to prevent notification fatigue, renders the template with personalization data, and enqueues the notification into channel-specific queues. The delivery layer has dedicated workers per channel: an Email Worker using a provider like SendGrid or SES, an SMS Worker using Twilio, and a Push Worker using APNs and FCM. Each worker implements retry logic with exponential backoff and a dead-letter queue for persistent failures. Use a priority queue so critical notifications (security alerts, OTP codes) bypass lower-priority marketing messages. Store notification history and delivery status in a database for analytics and audit. For deduplication, generate an idempotency key per notification and check a Redis cache before processing. For scale, partition Kafka topics by notification channel so each consumer group handles one channel independently. Monitor delivery rates, bounce rates, and latencies with dashboards and alerting. To handle provider outages, implement a fallback mechanism that routes through an alternative provider.",
                Difficulty: "Senior",
                Tags: ["notification-service", "message-queue", "scalability", "reliability"]),
            new(
                Question: "Design a distributed cache system. How do you handle cache invalidation and consistency?",
                ExpectedAnswer: "A distributed cache stores frequently accessed data across multiple nodes to reduce database load and latency. Use consistent hashing to distribute keys across cache nodes — this minimizes key redistribution when nodes are added or removed. Each key hashes to a position on a virtual ring and is stored on the next node clockwise. Virtual nodes (multiple hash positions per physical node) ensure even distribution. For cache invalidation, there are three primary strategies. Cache-Aside (Lazy Loading): the application checks the cache first; on miss, it reads from the database, writes to cache, and returns. Invalidation is done by deleting the cache key when the underlying data changes, and the next read repopulates it. Write-Through: every write goes to both the cache and database synchronously, ensuring consistency but adding write latency. Write-Behind (Write-Back): writes go to cache first and are asynchronously flushed to the database, offering low write latency but risking data loss if the cache node fails before flush. For consistency, use TTL as a safety net even with active invalidation — if invalidation fails, the stale entry expires. For strong consistency, use lease-based approaches where the cache issues a lease token on a miss, and the client must present it when setting the value, preventing thundering herd and stale sets. To prevent cache stampede when a hot key expires and hundreds of requests hit the database simultaneously, use a locking mechanism where only one request fetches from the database while others wait. Redis Cluster is a common implementation offering hash-slot-based partitioning, replication, and automatic failover.",
                Difficulty: "Senior",
                Tags: ["distributed-cache", "consistent-hashing", "cache-invalidation", "write-through"]),
            new(
                Question: "Explain the CAP theorem and its practical implications for distributed system design.",
                ExpectedAnswer: "The CAP theorem states that a distributed system can provide at most two of three guarantees simultaneously: Consistency (every read returns the most recent write), Availability (every request receives a non-error response), and Partition tolerance (the system operates despite network partitions between nodes). Since network partitions are inevitable in distributed systems, the real choice is between CP and AP during a partition. A CP system (like ZooKeeper, etcd, or MongoDB with majority write concern) sacrifices availability during a partition — nodes that cannot confirm they have the latest data will refuse requests rather than serve stale data. An AP system (like Cassandra, DynamoDB, or CouchDB) remains available during a partition but may serve stale data, relying on eventual consistency to reconcile after the partition heals. In practice, most systems make nuanced trade-offs rather than being purely CP or AP. For example, you might use strong consistency for financial transactions (CP) but eventual consistency for social media feeds (AP). DynamoDB lets you choose per-request between strongly consistent and eventually consistent reads. It is important to understand that CAP applies during partition events only — when the network is healthy, a well-designed system provides both consistency and availability. The PACELC extension adds that even in the absence of partition (Else), there is a trade-off between Latency and Consistency.",
                Difficulty: "Senior",
                Tags: ["cap-theorem", "distributed-systems", "consistency", "availability"]),
            new(
                Question: "What is CQRS and when would you apply it? How does it relate to event sourcing?",
                ExpectedAnswer: "CQRS (Command Query Responsibility Segregation) separates the write model (commands) from the read model (queries) into distinct models, potentially with separate databases. Commands mutate state and return no data; queries return data and cause no side effects. The write side validates and processes business operations using a normalized domain model, while the read side uses denormalized projections optimized for specific query patterns. Apply CQRS when read and write workloads have very different performance or scaling requirements, when the domain is complex with different representations needed for reads versus writes, or when you want to scale reads independently by adding read replicas. CQRS pairs naturally with event sourcing, where instead of storing current state, you store a sequence of domain events that produced the state. The write side appends events to an event store, and the read side subscribes to these events and projects them into read-optimized views (materialized views). This combination provides a complete audit trail, temporal queries (state at any point in time), and the ability to rebuild read models by replaying events. However, CQRS adds complexity: eventual consistency between write and read models, the need to handle stale reads in the UI, and more infrastructure. Do not apply CQRS to simple CRUD applications where the overhead is not justified. In .NET, MediatR is commonly used to implement the command/query dispatch pattern.",
                Difficulty: "Senior",
                Tags: ["cqrs", "event-sourcing", "architecture-pattern", "mediatr"]),
            new(
                Question: "How do you design a rate limiter? Compare different algorithms and their trade-offs.",
                ExpectedAnswer: "A rate limiter controls the rate of requests a client can make within a time window. There are several algorithms. Token Bucket: a bucket holds tokens up to a maximum capacity, tokens are added at a fixed rate, and each request consumes one token. If the bucket is empty, the request is rejected. This allows short bursts up to bucket capacity while maintaining an average rate. Leaking Bucket: requests enter a FIFO queue processed at a fixed rate. Excess requests are dropped. This produces a smooth, constant output rate but does not allow bursts. Fixed Window Counter: divide time into fixed windows and count requests per window. Simple but has a boundary problem — a burst at the end of one window and start of the next effectively doubles the rate. Sliding Window Log: store the timestamp of each request and count requests in the trailing window. Accurate but memory-intensive. Sliding Window Counter: combines fixed window and sliding log — uses the weighted count from the previous window plus the current window count, approximating a true sliding window with low memory. For distributed rate limiting, store counters in Redis using INCR with EXPIRE for fixed windows, or sorted sets for sliding window log. Use Lua scripts for atomicity. In an API gateway architecture, place the rate limiter at the gateway layer using client IP or API key as the identifier. Return HTTP 429 (Too Many Requests) with Retry-After header when the limit is exceeded. In ASP.NET Core, the built-in rate limiting middleware (Microsoft.AspNetCore.RateLimiting) supports fixed window, sliding window, token bucket, and concurrency limiter algorithms.",
                Difficulty: "Senior",
                Tags: ["rate-limiter", "token-bucket", "sliding-window", "distributed-systems"]),
            new(
                Question: "Compare microservices communication patterns: synchronous REST/gRPC versus asynchronous messaging.",
                ExpectedAnswer: "Synchronous communication (REST, gRPC) provides request-response semantics where the caller waits for a response. REST over HTTP is simple, widely supported, and human-readable but has overhead from text-based serialization and HTTP/1.1 connection management. gRPC uses HTTP/2 with Protocol Buffers for binary serialization, offering lower latency, streaming support, and strongly-typed contracts with code generation. gRPC is ideal for internal service-to-service calls where performance matters. The downside of synchronous communication is temporal coupling — if the downstream service is slow or down, the caller is directly affected. This creates cascading failures, which must be mitigated with circuit breakers, retries with exponential backoff, timeouts, and bulkheads. Asynchronous messaging (RabbitMQ, Kafka, Azure Service Bus) decouples services in time and availability. The sender publishes a message and continues without waiting. This improves resilience and enables event-driven architectures. Message queues support point-to-point (one consumer processes each message) and pub/sub (multiple consumers receive a copy) patterns. The trade-offs include eventual consistency, increased complexity in error handling (dead-letter queues, idempotent consumers, message ordering), and operational overhead of managing broker infrastructure. The Saga pattern coordinates multi-service transactions using either choreography (services react to events) or orchestration (a central coordinator directs the flow). In practice, most microservices architectures use a mix: synchronous for queries needing immediate responses and asynchronous for commands and event propagation.",
                Difficulty: "Senior",
                Tags: ["microservices", "grpc", "rest", "message-queue", "saga"]),
            new(
                Question: "Explain database sharding strategies and when you would choose each approach.",
                ExpectedAnswer: "Sharding horizontally partitions data across multiple database instances (shards) so each shard holds a subset of the data. This enables horizontal scaling beyond the capacity of a single database server. Range-based sharding assigns rows to shards based on a range of the shard key (e.g., user IDs 1-1M on shard 1, 1M-2M on shard 2). It is simple and supports range queries efficiently but can create hotspots if access patterns are skewed toward recent data. Hash-based sharding applies a hash function to the shard key and mods by the number of shards. This distributes data evenly but makes range queries impossible and requires reshuffling when adding shards. Consistent hashing mitigates reshuffling by mapping both keys and shards onto a ring. Directory-based sharding uses a lookup table that maps each key to a shard, offering maximum flexibility but introducing a single point of failure in the directory service. Key design considerations: choose a shard key with high cardinality and even distribution, typically a user ID or tenant ID. Avoid cross-shard queries and joins — design the data model so that related data lives on the same shard (co-locate by tenant). Cross-shard transactions require two-phase commit or saga patterns, adding complexity. Resharding (changing the number of shards) is operationally expensive. Consider using a database that supports native sharding like CockroachDB, Citus for PostgreSQL, or Cosmos DB, which handle shard management automatically. Start with read replicas and vertical scaling before resorting to sharding, as sharding significantly increases operational complexity.",
                Difficulty: "Staff",
                Tags: ["database-sharding", "horizontal-scaling", "consistent-hashing", "partitioning"]),
            new(
                Question: "What is the event-driven architecture pattern and how do you ensure reliability in event processing?",
                ExpectedAnswer: "Event-driven architecture (EDA) is a design pattern where components communicate by producing and consuming events — immutable records of something that happened. Producers emit events without knowing who consumes them, and consumers react independently, creating loose coupling. The core components are event producers, an event broker (Kafka, RabbitMQ, Azure Event Hubs), and event consumers. Events can be notifications (something happened, no payload needed), event-carried state transfer (event contains the data needed by consumers), or domain events (meaningful business occurrences). For reliability, implement the Transactional Outbox pattern: instead of publishing an event directly, write it to an outbox table in the same database transaction as the state change, then a separate process reads the outbox and publishes to the broker. This guarantees at-least-once delivery because the event is committed atomically with the state change. Consumers must be idempotent since messages can be delivered more than once — use a processed-event table or idempotency key to detect duplicates. For ordering, Kafka guarantees order within a partition, so use the entity ID as the partition key to ensure all events for an entity are processed in order. Implement dead-letter queues for messages that fail processing after retries. Monitor consumer lag to detect when consumers fall behind. Use schema registries (like Confluent Schema Registry) for event schema evolution with backward compatibility. Change Data Capture (CDC) with tools like Debezium is an alternative that captures database changes as events without modifying application code.",
                Difficulty: "Staff",
                Tags: ["event-driven", "transactional-outbox", "kafka", "reliability"]),
        ],
        KeyConcepts:
        [
            "Load balancing",
            "Horizontal scaling",
            "Database partitioning and sharding",
            "Message queues and event streaming",
            "Event sourcing",
            "Circuit breaker pattern",
            "Saga pattern (choreography and orchestration)",
            "Consistent hashing",
            "CAP theorem and PACELC",
            "CQRS",
        ],
        Resources:
        [
            new("Designing Data-Intensive Applications by Martin Kleppmann", "Book", "https://dataintensive.net/"),
            new("System Design Interview by Alex Xu", "Book", "https://www.amazon.com/System-Design-Interview-insiders-Second/dp/B08CMF2CQF"),
            new("Microsoft Azure Architecture Center", "Documentation", "https://learn.microsoft.com/en-us/azure/architecture/"),
            new("ByteByteGo System Design", "Video", "https://www.youtube.com/c/ByteByteGo"),
        ]);
    // ───────────────────────────────────────────────
    // Topic 3: .NET Internals
    // ───────────────────────────────────────────────
    private static TopicArea BuildDotNetInternals() => new(
        Id: "dotnet-internals",
        Name: ".NET Internals",
        Description: "Deep understanding of the Common Language Runtime, garbage collection, JIT compilation, memory layout, and low-level .NET mechanisms that impact application performance and correctness.",
        Questions:
        [
            new(
                Question: "Explain the CLR execution model from source code to running machine code.",
                ExpectedAnswer: "The C# compiler (Roslyn) compiles source code into an assembly containing Intermediate Language (IL) code and metadata. IL is a CPU-independent instruction set. When the application starts, the CLR loads the assembly and the JIT (Just-In-Time) compiler converts IL to native machine code method-by-method on first invocation. The JIT-compiled code is cached in memory so subsequent calls execute the native code directly without re-compilation. The CLR provides several services: the garbage collector manages memory, the type system enforces type safety, the exception handling mechanism provides structured exception support, and the thread pool manages worker threads. With .NET's tiered compilation, methods are initially compiled quickly with minimal optimizations (Tier 0), and after being called enough times (typically 30 invocations), they are recompiled with full optimizations (Tier 1). On-Stack Replacement (OSR) in .NET 7+ allows long-running methods with loops to be upgraded to optimized versions while still executing. Profile-Guided Optimization (PGO) in .NET 8+ uses runtime profiling data from Tier 0 execution to inform Tier 1 optimizations like devirtualization, hot/cold code separation, and branch prediction hints. NativeAOT compiles IL directly to native code at build time, eliminating JIT startup cost but losing runtime optimization capabilities.",
                Difficulty: "Mid",
                Tags: ["clr", "jit", "il", "tiered-compilation"]),
            new(
                Question: "Describe the .NET garbage collector's generational model, including GC roots, generations, and segments.",
                ExpectedAnswer: "The .NET GC is a tracing, generational, mark-and-sweep collector. It starts from GC roots — static fields, local variables on thread stacks, CPU registers, GC handles, and finalizer queue references — and traces all reachable objects. Unreachable objects are collected. The generational hypothesis states that most objects are short-lived, so the GC divides the managed heap into three generations. Gen 0 is small (typically a few MB per core) and collects frequently — most short-lived objects die here. Survivors are promoted to Gen 1, which acts as a buffer between short-lived and long-lived objects. Gen 1 collections are less frequent. Objects surviving Gen 1 are promoted to Gen 2, which holds long-lived objects and is collected infrequently with a full GC. The Large Object Heap (LOH, for objects 85,000+ bytes) is collected only with Gen 2 and is not compacted by default, which can cause fragmentation. The Pinned Object Heap (POH, .NET 5+) stores pinned objects separately to avoid fragmenting the regular heap. The GC uses segments: ephemeral segments hold Gen 0 and Gen 1, while Gen 2 and LOH have their own segments. Workstation GC uses a single GC thread and is optimized for latency. Server GC uses one GC thread per logical processor, each with its own heap, optimized for throughput. Background GC allows Gen 0/1 collections while a Gen 2 collection is in progress, reducing pause times.",
                Difficulty: "Senior",
                Tags: ["garbage-collection", "gc-generations", "loh", "gc-roots"]),
            new(
                Question: "What is the memory layout difference between value types and reference types in .NET?",
                ExpectedAnswer: "Reference types (classes) are allocated on the managed heap. Each object has an object header (8 bytes on x64) containing the sync block index (used for locking and COM interop) and a method table pointer (8 bytes on x64) pointing to type metadata. The instance fields follow. So even an empty class consumes at least 24 bytes on x64 (header + method table pointer + minimum allocation size padding, rounded to 8-byte alignment). Variables of reference types store a pointer (8 bytes on x64) to the heap object. Value types (structs) are stored inline — on the stack for local variables, or directly embedded within the containing object for fields. They have no object header or method table pointer, so a struct with a single int field uses just 4 bytes. This is why structs are more memory-efficient for small data. When a value type is boxed (cast to object or an interface), the CLR allocates a heap object with the standard object header and copies the value into it. This is why boxing is expensive and should be avoided on hot paths. Value types are copied on assignment, whereas reference types copy only the pointer. Structs larger than roughly 16 bytes should be passed by reference (using 'in', 'ref', or 'ref readonly') to avoid expensive copies. Arrays of value types store elements inline contiguously, which is cache-friendly, while arrays of reference types store pointers that must be dereferenced, causing cache misses.",
                Difficulty: "Mid",
                Tags: ["value-types", "reference-types", "memory-layout", "boxing"]),
            new(
                Question: "Explain Span<T> and Memory<T>. Why can Span<T> only live on the stack?",
                ExpectedAnswer: "Span<T> is a ref struct that provides a type-safe, bounds-checked view over a contiguous region of memory — managed arrays, stack-allocated buffers (stackalloc), or native memory. It consists of a managed pointer (ref T) and a length. Because it contains a ref field (a byref-like type), it is a ref struct and can only live on the stack. It cannot be stored as a field in a class, boxed, captured by a lambda, or used in async methods, because the GC cannot track managed pointers stored on the heap — the referenced memory might be moved or freed before the Span is accessed, causing dangling pointer issues. Memory<T> solves this restriction by wrapping an array reference, start index, and length — all regular fields that can be stored on the heap. Memory<T> can be used in async methods and stored as class fields. You convert Memory<T> to Span<T> via the .Span property when you need to perform the actual data access, but only in synchronous scope. The practical benefits are huge: parsing methods can accept ReadOnlySpan<char> and work efficiently on string slices without allocating substrings. Buffer operations can use Span<byte> to avoid array allocations. The stackalloc keyword combined with Span<T> allows allocating small buffers on the stack: Span<byte> buffer = stackalloc byte[256], which has zero GC pressure. The .NET runtime APIs extensively use Span for high-performance scenarios like UTF-8 encoding, number formatting, and I/O operations.",
                Difficulty: "Senior",
                Tags: ["span", "memory", "ref-struct", "stack-allocation"]),
            new(
                Question: "How does the async/await state machine work internally in C#?",
                ExpectedAnswer: "When the compiler encounters an async method, it rewrites it into a state machine — a struct (for methods returning ValueTask) or class implementing IAsyncStateMachine. The state machine has fields for each local variable that lives across await points, a state field tracking which await point was last reached, and an AsyncTaskMethodBuilder for managing the returned Task. When the method is called, it creates the state machine, sets state to -1, and calls MoveNext(). MoveNext() executes code until it hits an await. If the awaited task is already completed, it continues synchronously (the hot path that avoids allocation). If not, it captures the current ExecutionContext and SynchronizationContext, attaches a continuation callback (MoveNext itself) to the awaited task via GetAwaiter().OnCompleted(), and returns. When the awaited task completes, the continuation is invoked, MoveNext() is called again, the state field routes execution to resume after the last await point via a switch statement, and execution continues until the next await or the method completes. The struct-based state machine is initially on the stack, but if the first await does not complete synchronously, it is boxed to the heap. ValueTask<T> and pooled async builders (PoolingAsyncValueTaskMethodBuilder) reduce allocations for methods that frequently complete synchronously. The key takeaway is that each await point is a potential suspension point, and the compiler transforms sequential-looking code into a state machine that can pause and resume, while preserving the execution context across suspensions.",
                Difficulty: "Senior",
                Tags: ["async-await", "state-machine", "task", "continuation"]),
            new(
                Question: "Explain ThreadPool work stealing in .NET and how it manages work items.",
                ExpectedAnswer: "The .NET ThreadPool manages a pool of worker threads that execute work items (Task objects, QueueUserWorkItem callbacks, timer callbacks). Each worker thread has a local work queue (implemented as a lock-free deque — double-ended queue). When a task creates child tasks, they are pushed onto the local queue of the current thread. The thread processes items from its local queue in LIFO order (stack behavior), which improves cache locality because recently created items are likely to reference recently touched memory. When a thread's local queue is empty, it attempts to steal work from other threads' local queues in FIFO order (from the opposite end), which steals the oldest items that are less likely to be in the original thread's cache. There is also a global queue (FIFO) where externally queued items go. Threads check the global queue when they have no local work and nothing to steal. The ThreadPool dynamically adjusts the number of threads using a hill-climbing algorithm — it monitors throughput and adds or removes threads to maximize work completion. The minimum thread count defaults to the processor count. When more threads are needed beyond the minimum, new threads are injected slowly (one every 500ms) to avoid thread explosion. Task.Run enqueues to the global queue. Awaiting a task and then continuing typically places the continuation on the local queue of the thread that completed the awaited task. This work-stealing design ensures all CPUs remain busy with minimal contention because most work is processed from thread-local queues without locks.",
                Difficulty: "Senior",
                Tags: ["threadpool", "work-stealing", "concurrency", "task-scheduling"]),
            new(
                Question: "What are ref structs and what restrictions apply to them? Give practical examples.",
                ExpectedAnswer: "A ref struct is a value type that is guaranteed to exist only on the stack. It cannot be boxed, cannot be a field in a class or non-ref struct, cannot be captured by lambdas or used in async methods, cannot implement interfaces (prior to C# 13), and cannot be used as a generic type argument (prior to the allows ref struct anti-constraint in C# 13). These restrictions exist because ref structs can contain managed pointers (ref fields) that the garbage collector cannot track on the heap. The primary examples in .NET are Span<T> and ReadOnlySpan<T>, which hold a ref T field pointing into an array or other memory. Practical use cases include: high-performance parsers that slice through input data without allocating substrings, using Span<char> to tokenize text on the stack; custom enumerators that hold a Span<T> field for zero-allocation iteration; and combining stackalloc with Span<T> for temporary buffers. C# 11 introduced ref fields, allowing you to define your own ref structs with ref fields: 'ref struct MySpan { ref int _reference; int _length; }'. C# 13 relaxed some restrictions, allowing ref structs to implement interfaces and be used as type parameters when the parameter declares 'allows ref struct', enabling patterns like Span<T> to implement IEnumerable-like interfaces. The scoped keyword further restricts escape scope, letting the compiler guarantee a ref struct does not escape the current method even as a parameter.",
                Difficulty: "Senior",
                Tags: ["ref-struct", "span", "stack-only", "managed-pointers"]),
            new(
                Question: "How does assembly loading work in .NET, and what replaced AppDomains?",
                ExpectedAnswer: "In .NET (Core and later), each application has a single AppDomain — the multiple-AppDomain model from .NET Framework was removed. Assemblies are loaded by AssemblyLoadContext (ALC), which replaced AppDomain's isolation model. The default ALC loads the main application assemblies using the deps.json file and probing paths. You can create custom ALC instances to load assemblies into isolated contexts, enabling plug-in architectures where you load add-ins into a separate ALC and can unload them by unloading the ALC (the assemblies are collected when no references to their types remain and the ALC is collectible). This requires setting isCollectible: true when creating the ALC. Assembly resolution follows a chain: the runtime first checks if the assembly is already loaded, then the default ALC tries its configured paths, and if that fails, the ALC.Resolving event fires where custom logic can locate the assembly. The AssemblyDependencyResolver class helps resolve dependencies for plug-ins by reading their .deps.json files. One key difference from AppDomains is that ALCs do not provide security boundaries — for process-level isolation, use separate processes. The typical plug-in pattern creates a custom ALC subclass that overrides Load() to resolve the plug-in's dependencies from its directory, preventing them from conflicting with the host's dependencies. Type identity is tied to the ALC: the same assembly loaded in two different ALCs produces different Type instances, so objects cannot be cast across ALC boundaries without shared interfaces loaded from the default context.",
                Difficulty: "Senior",
                Tags: ["assembly-loading", "assembly-load-context", "plugins", "app-domain"]),
            new(
                Question: "Describe tiered compilation and Profile-Guided Optimization (PGO) in modern .NET.",
                ExpectedAnswer: "Tiered compilation, enabled by default since .NET Core 3.0, compiles methods in multiple stages. Tier 0 uses minimal JIT optimizations for fast startup — the method runs but may be slower than fully optimized code. The runtime counts method invocations, and after a threshold (around 30 calls), it queues the method for Tier 1 recompilation with full optimizations (inlining, loop optimizations, dead code elimination, register allocation). This provides a balance between startup latency and steady-state throughput. Quick JIT for loops (disabled by default) extends Tier 0 to methods containing loops, which can improve startup for applications that call many methods during initialization. On-Stack Replacement (OSR), introduced in .NET 7, allows a method running a long loop at Tier 0 to be replaced with an optimized Tier 1 version while the loop is still executing, without waiting for the method to be called again. Dynamic PGO, enabled by default in .NET 8, leverages instrumentation data collected during Tier 0 execution to guide Tier 1 optimizations. It profiles which virtual/interface methods are actually called (enabling guarded devirtualization — inlining the common case with a type check fallback), which branches are hot or cold (enabling better code layout), and type feedback for generic methods. Static PGO can use instrumentation data from previous runs (via DOTNET_TieredPGO and instrumented builds) to optimize ahead of time. Together, tiered compilation and dynamic PGO can achieve 10-20% throughput improvements over static Tier 1 compilation alone because the optimizations are tailored to actual runtime behavior.",
                Difficulty: "Staff",
                Tags: ["tiered-compilation", "pgo", "osr", "jit-optimization"]),
            new(
                Question: "What is NativeAOT and how does it differ from ReadyToRun and standard JIT compilation?",
                ExpectedAnswer: "NativeAOT (Ahead-of-Time) compiles .NET applications directly to native machine code at build time, producing a self-contained executable with no JIT compiler or IL in the final binary. The result is instant startup (no JIT delay), smaller deployment size (no runtime included beyond what is used), and reduced memory footprint. However, NativeAOT has significant limitations: no runtime code generation (no Reflection.Emit, no Assembly.LoadFrom for dynamic assemblies), limited reflection (only types that are statically reachable are preserved — use trimming annotations or rd.xml to keep others), no dynamic loading of assemblies, and certain LINQ expression compilation scenarios may not work. ReadyToRun (R2R) is a hybrid approach that precompiles assemblies to native code at publish time but includes the IL as a fallback. The precompiled code may not be as optimized as JIT Tier 1 because it cannot assume the exact CPU features at runtime, but it eliminates first-call JIT delays. The JIT can later recompile hot methods with full optimizations (tiered compilation still works). Standard JIT compilation compiles everything at runtime, allowing maximum optimization for the specific hardware but paying startup cost. Choose NativeAOT for CLI tools, serverless functions, and microservices where startup time is critical and the application does not use heavy reflection. Choose ReadyToRun for larger applications that want faster startup but need full runtime capabilities. Choose standard JIT when startup is not a concern and you want maximum steady-state performance with PGO.",
                Difficulty: "Senior",
                Tags: ["nativeaot", "ready-to-run", "ahead-of-time", "trimming"]),
        ],
        KeyConcepts:
        [
            "GC roots and reachability",
            "LOH/POH heap segments",
            "Tiered compilation and OSR",
            "Profile-Guided Optimization (PGO)",
            "NativeAOT compilation",
            "ref structs and stack-only types",
            "stackalloc and Span-based buffers",
            "Object header layout (sync block + method table pointer)",
            "AssemblyLoadContext and plug-in isolation",
            "Async state machine transformation",
        ],
        Resources:
        [
            new("Pro .NET Memory Management by Konrad Kokosa", "Book", "https://prodotnetmemory.com/"),
            new(".NET GC Documentation", "Documentation", "https://learn.microsoft.com/en-us/dotnet/standard/garbage-collection/"),
            new("Adam Sitnik — Span<T>", "Article", "https://adamsitnik.com/Span/"),
            new(".NET Runtime Source Code", "Documentation", "https://github.com/dotnet/runtime"),
        ]);
    // ───────────────────────────────────────────────
    // Topic 4: Backend Architecture
    // ───────────────────────────────────────────────
    private static TopicArea BuildBackendArchitecture() => new(
        Id: "backend-architecture",
        Name: "Backend Architecture",
        Description: "Software architecture patterns for building maintainable, scalable backend services in .NET, including Clean Architecture, domain-driven design, API design strategies, and ASP.NET Core middleware and hosting models.",
        Questions:
        [
            new(
                Question: "Explain Clean Architecture and its layers. How do dependencies flow?",
                ExpectedAnswer: "Clean Architecture, popularized by Robert C. Martin, organizes code into concentric layers where dependencies point inward — outer layers depend on inner layers, never the reverse. The innermost layer is the Domain (or Entities) layer, containing enterprise business rules, domain entities, value objects, and domain events with no dependencies on any framework or infrastructure. The Application (or Use Cases) layer contains application-specific business logic — command and query handlers, interfaces for infrastructure services (repositories, email senders), DTOs, and validation rules. It depends only on the Domain layer. The Infrastructure layer implements the interfaces defined in the Application layer: database repositories using EF Core, email services, file storage, external API clients. It depends on the Application layer's abstractions. The Presentation layer (API controllers, Razor Pages, or Blazor) depends on the Application layer to dispatch commands and queries. The key benefit is testability: the Application and Domain layers can be unit-tested without any infrastructure dependencies by mocking the interfaces. Dependency Inversion is the enabling principle — high-level modules define abstractions that low-level modules implement. In a .NET solution, this typically maps to projects: Domain.csproj (no project references), Application.csproj (references Domain), Infrastructure.csproj (references Application), and WebApi.csproj (references Application, registers Infrastructure via DI in Program.cs). The Presentation layer registers Infrastructure implementations against Application interfaces in the DI container, which is the Composition Root.",
                Difficulty: "Mid",
                Tags: ["clean-architecture", "dependency-inversion", "layers", "testability"]),
            new(
                Question: "What are DDD aggregates and bounded contexts? How do they influence microservice boundaries?",
                ExpectedAnswer: "In Domain-Driven Design, an Aggregate is a cluster of domain objects treated as a single unit for data consistency. The Aggregate Root is the only entity through which external code can interact with the aggregate. For example, an Order aggregate might contain OrderLines and a ShippingAddress — you always load and save the entire Order through the Order aggregate root, never modify an OrderLine independently. Aggregates enforce invariants: the Order root ensures that the total does not exceed a credit limit and that quantity is positive. Aggregates should be as small as possible — reference other aggregates by ID, not by direct object reference. A Bounded Context is a linguistic and model boundary within which a particular domain model applies. The same real-world concept (like 'Customer') may have different representations in different contexts: in the Sales context, Customer has a credit limit and order history; in the Shipping context, it has an address and delivery preferences. Each bounded context has its own ubiquitous language, its own model, and ideally its own database. Bounded contexts communicate through well-defined integration events or APIs, with an Anti-Corruption Layer (ACL) translating between models. Bounded contexts naturally map to microservice boundaries because they represent independently deployable units with their own data stores. However, not every bounded context needs to be a separate microservice — start with a modular monolith where bounded contexts are separate modules within one deployment, and extract to microservices only when independent scaling or deployment is needed.",
                Difficulty: "Senior",
                Tags: ["ddd", "aggregates", "bounded-context", "microservices"]),
            new(
                Question: "How do you implement CQRS with MediatR in an ASP.NET Core application?",
                ExpectedAnswer: "MediatR is a mediator library that decouples request senders from handlers. In a CQRS setup, define commands (write operations) and queries (read operations) as separate request types. Create a command record like 'public record CreateOrderCommand(string CustomerId, List<OrderItemDto> Items) : IRequest<Guid>' and its handler 'public class CreateOrderCommandHandler : IRequestHandler<CreateOrderCommand, Guid>' that contains the business logic, calls the repository, and returns the new order ID. Queries follow the same pattern: 'public record GetOrderQuery(Guid OrderId) : IRequest<OrderDto>' with a handler that reads from the database (potentially a read-optimized view or separate read database). Register MediatR in DI with builder.Services.AddMediatR(cfg => cfg.RegisterServicesFromAssembly(typeof(CreateOrderCommand).Assembly)). In controllers or minimal API endpoints, inject IMediator and call await mediator.Send(command). MediatR's pipeline behaviors (IPipelineBehavior<TRequest, TResponse>) act as middleware for cross-cutting concerns: add a ValidationBehavior that runs FluentValidation validators before the handler executes, a LoggingBehavior that logs all requests and durations, and a TransactionBehavior that wraps command handlers in a database transaction. The pipeline processes behaviors in registration order, then calls the handler. Separating commands from queries allows different optimization strategies: queries can use Dapper with raw SQL for performance while commands use EF Core for change tracking. You can also scale read and write sides independently — queries hitting read replicas and commands hitting the primary.",
                Difficulty: "Senior",
                Tags: ["cqrs", "mediatr", "pipeline-behaviors", "asp-net-core"]),
            new(
                Question: "Compare API versioning strategies in ASP.NET Core. Which approach would you recommend?",
                ExpectedAnswer: "There are four main API versioning strategies. URL path versioning (/api/v1/orders, /api/v2/orders) is the most explicit and discoverable — clients clearly see which version they are using. It breaks REST purists' principle that a resource should have one URI, but it is the most practical and widely adopted approach. Query string versioning (/api/orders?api-version=2.0) keeps the URL path clean but the version can be overlooked and is harder to route in API gateways. Header versioning (Api-Version: 2.0 custom header) keeps URLs completely clean but is not discoverable through browser navigation and requires tooling to test. Media type versioning (Accept: application/vnd.myapi.v2+json) is the most REST-compliant approach but is complex and rarely used in practice. In ASP.NET Core, use the Asp.Versioning.Http and Asp.Versioning.Mvc packages. Configure versioning in Program.cs with builder.Services.AddApiVersioning(options => { options.DefaultApiVersion = new ApiVersion(1, 0); options.AssumeDefaultVersionWhenUnspecified = true; options.ReportApiVersions = true; }). Decorate controllers with [ApiVersion(\"1.0\")] and [ApiVersion(\"2.0\")] and use [MapToApiVersion(\"2.0\")] on specific actions. My recommendation for most APIs is URL path versioning combined with a sunset policy: maintain at most two active versions, communicate deprecation timelines in response headers (api-deprecated-versions, api-supported-versions), and give clients 6-12 months to migrate. Use API version negotiation so new clients get the latest version by default.",
                Difficulty: "Mid",
                Tags: ["api-versioning", "rest-api", "asp-net-core", "backward-compatibility"]),
            new(
                Question: "How does the ASP.NET Core middleware pipeline work? Explain the request processing order.",
                ExpectedAnswer: "The ASP.NET Core middleware pipeline is a chain of delegates that process HTTP requests and responses. Each middleware component can perform work before and after calling the next middleware, or short-circuit the pipeline by not calling next. The pipeline is built in Program.cs using app.UseXxx() calls, and the order of registration determines the execution order. A request enters the pipeline from the top and flows through each middleware in order; the response flows back through them in reverse order (like nested Russian dolls). Typical ordering is: UseExceptionHandler (catch unhandled exceptions — must be first to catch exceptions from all downstream middleware), UseHsts, UseHttpsRedirection, UseStaticFiles (short-circuits for static files without hitting later middleware), UseRouting (determines which endpoint matches the request), UseCors, UseAuthentication (establishes the user identity from the token/cookie), UseAuthorization (checks if the authenticated user can access the endpoint), UseRateLimiter, UseResponseCaching, and finally UseEndpoints or MapControllers/MapGet (terminal middleware that executes the matched endpoint). Custom middleware can be written as a class with an InvokeAsync(HttpContext context, RequestDelegate next) method or inline with app.Use(async (context, next) => { /* before */ await next(context); /* after */ }). The key design insight is that middleware is composable and each component is unaware of others. UseRouting and UseEndpoints work together: UseRouting sets the endpoint on HttpContext, and middleware between them (like UseAuthorization) can inspect the selected endpoint's metadata before UseEndpoints executes it.",
                Difficulty: "Mid",
                Tags: ["middleware", "asp-net-core", "request-pipeline", "http"]),
            new(
                Question: "Compare minimal APIs versus controllers in ASP.NET Core. When would you choose each?",
                ExpectedAnswer: "Minimal APIs, introduced in .NET 6, define endpoints as lambda functions directly in Program.cs or using extension methods: app.MapGet(\"/orders/{id}\", async (int id, IOrderService service) => await service.GetOrder(id)). They have less ceremony, no controller classes, no base class inheritance, and automatic parameter binding from route, query, header, and body. They are ideal for microservices with a small number of endpoints, rapid prototyping, and APIs where the overhead of controllers is unnecessary. Controllers use the traditional MVC pattern with [ApiController]-attributed classes inheriting ControllerBase. They offer richer features out of the box: model validation with [Required] and [FromBody] annotations, content negotiation, OData support, API Explorer integration for Swagger, and built-in support for filters (action filters, result filters, exception filters). Controllers naturally organize related endpoints into cohesive classes and support inheritance for shared behavior. Choose minimal APIs for small services (under 20 endpoints), serverless functions, or when startup performance matters (minimal APIs have slightly less reflection overhead). Choose controllers for larger APIs with complex validation, content negotiation needs, or when you want a consistent organizational pattern for a team. In practice, the distinction has narrowed: minimal APIs in .NET 7+ support filters, route groups for organizing endpoints, and TypedResults for explicit return types. You can also use the Carter library to organize minimal APIs into modules that resemble controllers. Both approaches share the same routing, DI, middleware, and authentication infrastructure — the choice is primarily about code organization preference.",
                Difficulty: "Mid",
                Tags: ["minimal-apis", "controllers", "asp-net-core", "api-design"]),
            new(
                Question: "Compare gRPC, REST, and GraphQL for API design. When is each appropriate?",
                ExpectedAnswer: "REST is the most widely adopted API style, using HTTP methods on resource URLs. It is simple, cacheable (HTTP caching works naturally with GET requests), and universally supported. REST is ideal for public APIs, CRUD-centric services, and when broad client compatibility matters (browsers, mobile, third-party integrations). Downsides include over-fetching (getting more fields than needed) and under-fetching (requiring multiple requests to assemble a view). gRPC uses HTTP/2 with Protocol Buffers for binary serialization, providing 5-10x smaller payloads and faster serialization than JSON. It supports four communication patterns: unary (request-response), server streaming, client streaming, and bidirectional streaming. gRPC generates strongly-typed client and server stubs from .proto files. It is ideal for internal service-to-service communication where performance matters, streaming use cases, and polyglot environments where contract-first design ensures compatibility. Downsides include lack of browser support without gRPC-Web (a proxy layer), non-human-readable payloads for debugging, and less tooling for testing compared to REST. GraphQL exposes a single endpoint with a typed schema, and clients specify exactly which fields they need in their query. This solves over-fetching and under-fetching and is excellent for client-driven data requirements, mobile apps with bandwidth constraints, and aggregating data from multiple backend services through a single gateway. Downsides include complexity in caching (POST requests are not cacheable by default), potential for expensive queries (require depth and complexity limiting), and the N+1 problem requiring DataLoader batching. In ASP.NET Core, use MapGrpcService for gRPC, MapControllers or MapGet for REST, and Hot Chocolate or GraphQL.NET for GraphQL.",
                Difficulty: "Senior",
                Tags: ["grpc", "rest", "graphql", "api-design", "protocol-buffers"]),
            new(
                Question: "Explain health checks and readiness probes in ASP.NET Core. How do they integrate with Kubernetes?",
                ExpectedAnswer: "Health checks report whether an application and its dependencies are functioning correctly. ASP.NET Core provides a built-in health checks framework via builder.Services.AddHealthChecks() and app.MapHealthChecks(\"/health\"). You register checks for dependencies: .AddSqlServer(connectionString) for database connectivity, .AddRedis(redisConnectionString) for cache, .AddUrlGroup(new Uri(\"https://api.external.com\")) for external services. Each check returns Healthy, Degraded, or Unhealthy. In Kubernetes, two probe types matter: the liveness probe checks if the application process is alive and should be restarted if unhealthy — map this to a lightweight endpoint that checks only the application process (not dependencies, because a database outage should not cause pod restarts). The readiness probe checks if the application can serve traffic — map this to a comprehensive check including database, cache, and message broker connectivity. When readiness fails, Kubernetes removes the pod from the service's endpoints so traffic routes to healthy pods, but does not restart it. Configure separate endpoints: app.MapHealthChecks(\"/health/live\", new HealthCheckOptions { Predicate = _ => false }) for liveness (no dependency checks) and app.MapHealthChecks(\"/health/ready\", new HealthCheckOptions { Predicate = check => check.Tags.Contains(\"ready\") }) for readiness. Use the AspNetCore.HealthChecks.UI NuGet package for a dashboard visualizing health status across services. Add tags to categorize checks and filter them per endpoint. Set timeouts on individual checks to prevent a slow dependency from making the health endpoint itself unresponsive.",
                Difficulty: "Mid",
                Tags: ["health-checks", "kubernetes", "readiness-probe", "liveness-probe"]),
            new(
                Question: "What is the Strangler Fig pattern and how would you apply it to migrate a legacy .NET Framework application?",
                ExpectedAnswer: "The Strangler Fig pattern incrementally replaces a legacy system by routing specific functionality to a new system while the old system continues handling everything else. Over time, more functionality is migrated until the legacy system is fully replaced. For migrating a .NET Framework application to .NET 8+, start by placing a reverse proxy (YARP, which is a .NET-based reverse proxy, or nginx) in front of both the legacy application and the new .NET 8 application. Initially, all traffic routes to the legacy app. Identify a bounded context or feature area to migrate first — choose something with clear boundaries and high business value or frequent changes. Rewrite that feature in the new .NET 8 application, ensuring API compatibility. Update the proxy routing rules to direct requests for the migrated endpoints to the new service. The legacy app continues handling all other requests. Repeat for each feature area. For shared state, you may need an Anti-Corruption Layer that translates between the old and new models, or a shared database during the transition (though this creates coupling). The YARP reverse proxy is particularly suited for .NET migrations because it is highly configurable, supports header-based and path-based routing, load balancing, and can be hosted in the new .NET 8 application itself. Key principles: never do a big-bang rewrite (high risk of failure), always maintain the ability to roll back by keeping the legacy route available, and write integration tests that verify both systems produce identical results for the migrated functionality before cutting over.",
                Difficulty: "Staff",
                Tags: ["strangler-fig", "migration", "legacy", "yarp", "reverse-proxy"]),
            new(
                Question: "Describe the Backend for Frontend (BFF) pattern and API gateway patterns.",
                ExpectedAnswer: "The Backend for Frontend (BFF) pattern creates a dedicated backend service tailored to each frontend client type. A mobile BFF returns compact payloads optimized for bandwidth constraints, a web BFF returns richer data suited for desktop browsers, and an admin BFF exposes management endpoints. Each BFF aggregates calls to downstream microservices and shapes the response for its specific client, eliminating over-fetching and reducing round trips. The BFF owns the session and authentication flow for its client — for example, a web BFF might handle OAuth authorization code flow with cookie-based sessions, while a mobile BFF handles device token authentication. An API Gateway sits between clients and backend services, providing cross-cutting concerns: routing requests to the correct microservice, authentication and authorization, rate limiting, response caching, request/response transformation, load balancing, and circuit breaking. In .NET, YARP (Yet Another Reverse Proxy) or Ocelot can serve as API gateways. The distinction is that an API gateway is a shared infrastructure component, while a BFF is an application-level component with business logic specific to a client type. Avoid making the API gateway too smart — if it contains business logic or data transformation, it becomes a bottleneck and a coupling point. Keep the gateway thin (routing, auth, rate limiting) and push data aggregation to BFF services. In practice, a common architecture uses a thin API gateway for cross-cutting concerns with BFF services behind it that aggregate and transform data for specific clients, calling downstream microservices through the gateway or directly via service mesh.",
                Difficulty: "Senior",
                Tags: ["bff", "api-gateway", "yarp", "microservices", "aggregation"]),
        ],
        KeyConcepts:
        [
            "Onion/Hexagonal/Clean Architecture",
            "Domain events and aggregates",
            "Anti-corruption layer",
            "Strangler Fig migration pattern",
            "BFF (Backend for Frontend) pattern",
            "API gateway and reverse proxy",
            "CQRS with MediatR pipeline",
            "Middleware pipeline ordering",
            "API versioning strategies",
            "Health checks and Kubernetes probes",
        ],
        Resources:
        [
            new("Clean Architecture by Robert C. Martin", "Book", "https://www.amazon.com/Clean-Architecture-Craftsmans-Software-Structure/dp/0134494164"),
            new("ASP.NET Core Documentation", "Documentation", "https://learn.microsoft.com/en-us/aspnet/core/"),
            new("Domain-Driven Design by Eric Evans", "Book", "https://www.domainlanguage.com/ddd/"),
            new("YARP Reverse Proxy", "Documentation", "https://microsoft.github.io/reverse-proxy/"),
        ]);
    // ───────────────────────────────────────────────
    // Topic 5: Databases
    // ───────────────────────────────────────────────
    private static TopicArea BuildDatabases() => new(
        Id: "databases",
        Name: "Databases",
        Description: "Relational and NoSQL database fundamentals including query optimization, indexing strategies, transaction management, and choosing the right database technology for different workloads.",
        Questions:
        [
            new(
                Question: "How do you read and optimize a SQL Server execution plan? What are the key warning signs?",
                ExpectedAnswer: "An execution plan shows the query optimizer's chosen strategy for executing a query. Read it right-to-left, bottom-to-top — data flows from right to left, and the final result is at the top-left. Each operator shows its estimated cost as a percentage of the total. Key operators to understand: Index Seek (good — targets specific rows using the index B-tree), Index Scan (potentially bad — reads the entire index, acceptable only if you need most rows), Table Scan (usually bad — reads the entire heap, indicates a missing index), Nested Loop Join (efficient for small outer inputs), Hash Match Join (used when joining large unsorted datasets — expensive memory grant), Merge Join (efficient when both inputs are sorted on the join key), and Sort operator (expensive if unexpected — consider adding an index that provides the order). Warning signs include: thick arrows indicating large row counts flowing between operators, yellow triangle warnings indicating implicit conversions (a WHERE clause comparing varchar to nvarchar forces a scan instead of seek), missing index suggestions in green text, Key Lookup or RID Lookup operators (the query found the row via a non-clustered index but needs columns not in that index, requiring a trip back to the clustered index — solve with a covering index using INCLUDE columns), Spills to tempdb on Sort or Hash operators indicating insufficient memory grants, and estimated versus actual row count discrepancies suggesting stale statistics. Use SET STATISTICS IO ON to see logical reads — reducing logical reads is the single best optimization metric.",
                Difficulty: "Senior",
                Tags: ["execution-plan", "sql-server", "query-optimization", "indexing"]),
            new(
                Question: "Explain indexing strategies: covering indexes, composite indexes, and filtered indexes.",
                ExpectedAnswer: "A composite index is an index on multiple columns. Column order matters critically — the index can be used for queries that filter or sort on a leftmost prefix of the columns. An index on (LastName, FirstName, DateOfBirth) supports queries filtering on LastName alone, or LastName and FirstName, but not FirstName alone. Place the most selective column (highest cardinality, most commonly filtered) first, but also consider sort order and range scan requirements — columns used in equality predicates should come before columns used in range predicates. A covering index includes all columns needed by a query, so the query can be answered entirely from the index without accessing the base table (a Key Lookup). Use the INCLUDE clause to add non-key columns to the leaf level of a non-clustered index without affecting the index's sort order. For example, CREATE NONCLUSTERED INDEX IX_Orders_CustomerId ON Orders(CustomerId) INCLUDE (OrderDate, TotalAmount) allows queries filtering by CustomerId and selecting OrderDate and TotalAmount to be served entirely from the index. A filtered index has a WHERE clause, indexing only a subset of rows. CREATE NONCLUSTERED INDEX IX_Orders_Active ON Orders(OrderDate) WHERE Status = 'Active' creates a smaller, more efficient index for queries that filter on active orders. Filtered indexes are excellent for columns with skewed data distributions — if 95% of orders are completed and you frequently query the 5% that are active, a filtered index is much smaller and faster. Index maintenance considerations: too many indexes slow down writes because every INSERT, UPDATE, and DELETE must update all affected indexes. Monitor index usage via sys.dm_db_index_usage_stats and drop unused indexes.",
                Difficulty: "Senior",
                Tags: ["indexing", "covering-index", "composite-index", "filtered-index"]),
            new(
                Question: "Describe the four SQL transaction isolation levels and the anomalies each prevents.",
                ExpectedAnswer: "Transaction isolation levels control the degree of locking and row versioning, balancing data consistency against concurrency. Read Uncommitted is the lowest level — transactions can read data modified by other uncommitted transactions (dirty reads). It uses no shared locks on reads, offering maximum concurrency but risking reading data that gets rolled back. Use it only for approximate analytics where consistency does not matter. Read Committed is the default in SQL Server — it prevents dirty reads by holding shared locks during read operations, but releases them immediately, so a row read at the beginning of a transaction might return different data if read again later (non-repeatable reads). Other transactions can insert new rows that match your WHERE clause between two executions of the same query (phantom reads). Repeatable Read prevents dirty reads and non-repeatable reads by holding shared locks on all read rows until the transaction ends. However, phantoms are still possible because the locks are on rows, not on the range. Serializable prevents all three anomalies — dirty reads, non-repeatable reads, and phantom reads — by taking range locks that prevent other transactions from inserting, updating, or deleting rows that would fall within the ranges accessed by the transaction. It provides full isolation but at the cost of significant concurrency reduction and increased deadlock potential. SQL Server also offers Read Committed Snapshot Isolation (RCSI) and Snapshot Isolation, which use row versioning in tempdb instead of locking. RCSI gives each statement a point-in-time view without blocking writers, and Snapshot Isolation gives each transaction a consistent point-in-time view from the start of the transaction. PostgreSQL uses MVCC (Multi-Version Concurrency Control) by default, where readers never block writers and vice versa.",
                Difficulty: "Senior",
                Tags: ["isolation-levels", "transactions", "acid", "mvcc", "locking"]),
            new(
                Question: "How do you detect and prevent deadlocks in a relational database?",
                ExpectedAnswer: "A deadlock occurs when two or more transactions hold locks that the other needs, creating a circular wait. Transaction A locks Row 1 and waits for Row 2; Transaction B locks Row 2 and waits for Row 1 — neither can proceed. SQL Server automatically detects deadlocks using a background thread that periodically checks for cycles in the wait-for graph. It chooses the cheapest transaction to roll back as the deadlock victim (based on log bytes written). Detection: enable trace flag 1222 or use Extended Events (system_health session captures deadlock graphs by default in modern SQL Server). The deadlock graph XML shows the processes involved, the locks they held and waited for, and the SQL statements. Prevention strategies include: accessing tables and rows in a consistent order across all transactions (if all code accesses the Customer table before the Order table, circular waits cannot form); keeping transactions as short as possible to minimize lock duration; using the lowest necessary isolation level (Read Committed Snapshot avoids many lock-based deadlocks); adding appropriate indexes so queries lock fewer rows (a table scan locks every row in the table); using UPDLOCK hint when reading rows you intend to update, which takes an update lock immediately instead of escalating from shared to exclusive later; and implementing retry logic in the application — catch SqlException with error number 1205 (deadlock victim) and retry the transaction. In high-contention scenarios, consider optimistic concurrency with row versioning instead of pessimistic locking.",
                Difficulty: "Senior",
                Tags: ["deadlocks", "locking", "sql-server", "concurrency"]),
            new(
                Question: "Compare PostgreSQL and SQL Server for a .NET application. What are the key differences?",
                ExpectedAnswer: "Both are mature, ACID-compliant relational databases with strong .NET support via Npgsql (PostgreSQL) and Microsoft.Data.SqlClient (SQL Server). PostgreSQL uses MVCC by default — readers never block writers, providing better concurrency without needing explicit Snapshot Isolation configuration. SQL Server defaults to lock-based Read Committed and requires enabling RCSI or Snapshot Isolation for MVCC-like behavior. PostgreSQL has richer data types: native JSON/JSONB with indexing and querying, arrays, range types, hstore (key-value), PostGIS for geospatial, and user-defined types. SQL Server has JSON support (improved in SQL Server 2022 with JSON data type) but less mature. PostgreSQL supports table inheritance, partial indexes (equivalent to SQL Server's filtered indexes), and expression indexes (index on a computed expression). SQL Server has unique strengths: superior tooling with SQL Server Management Studio and Azure Data Studio, columnstore indexes for analytics, In-Memory OLTP (Hekaton) for extreme throughput on hot tables, built-in full-text search (PostgreSQL also has this), temporal tables (system-versioned tables for historical data tracking), and deep Azure integration with Azure SQL Database. Licensing is a major differentiator: PostgreSQL is free and open source, while SQL Server requires per-core licensing that can be very expensive for large deployments. For .NET specifically, EF Core has equally strong support for both via Npgsql.EntityFrameworkCore.PostgreSQL and Microsoft.EntityFrameworkCore.SqlServer. Performance is comparable for most OLTP workloads. Choose PostgreSQL for cost-sensitive projects, complex data types, and open-source preference. Choose SQL Server for enterprise environments already invested in the Microsoft ecosystem, when Azure SQL managed service is desired, or when In-Memory OLTP or columnstore is needed.",
                Difficulty: "Mid",
                Tags: ["postgresql", "sql-server", "database-comparison", "mvcc"]),
            new(
                Question: "When would you use a NoSQL database like MongoDB or CosmosDB instead of a relational database?",
                ExpectedAnswer: "NoSQL databases excel in specific scenarios. Use a document database (MongoDB, CosmosDB) when your data is naturally hierarchical or denormalized — for example, a product catalog where each product has varying attributes, or a content management system where documents have flexible schemas. Document databases store related data together in a single document, eliminating joins and providing fast single-document reads. Use them when you need horizontal scalability across commodity servers — MongoDB's sharding and CosmosDB's automatic partitioning distribute data across nodes transparently. Use them when your schema evolves frequently — adding a field to a document does not require ALTER TABLE on millions of rows. CosmosDB specifically offers multi-model (document, graph, key-value, column-family) with global distribution, single-digit-millisecond latency guarantees, and five tunable consistency levels from strong to eventual. MongoDB provides a powerful aggregation pipeline, change streams for real-time notifications, and flexible indexing including text, geospatial, and wildcard indexes. However, relational databases are better when you need complex transactions spanning multiple entities (ACID across tables), complex joins and ad-hoc queries, strong consistency by default, and mature tooling for reporting and analytics. The biggest anti-pattern with NoSQL is treating it like a relational database — modeling highly relational data with references between collections and performing client-side joins, which is slower than a SQL JOIN. Design NoSQL schemas around access patterns: embed related data that is read together, and reference data that is read independently. In .NET, use MongoDB.Driver for MongoDB and Microsoft.Azure.Cosmos for CosmosDB, both with excellent LINQ support.",
                Difficulty: "Mid",
                Tags: ["nosql", "mongodb", "cosmosdb", "document-database", "schema-design"]),
            new(
                Question: "Explain Redis data structures and their use cases beyond simple key-value caching.",
                ExpectedAnswer: "Redis is an in-memory data structure server offering much more than simple string caching. Strings are the basic type: use for caching serialized objects, counters (INCR/DECR are atomic), and distributed locks (SET with NX and EX flags for lock-with-expiry). Hashes store field-value pairs under a key — ideal for caching objects where you need to update individual fields without deserializing the entire object: HSET user:1001 name 'Alice' email 'alice@example.com'. Lists are linked lists supporting push/pop from both ends — use for message queues (LPUSH/BRPOP pattern), activity feeds (LPUSH recent items, LTRIM to cap length), and task queues. Sets store unique unordered elements — use for tracking unique visitors (SADD), tag systems, and computing intersections/unions across sets (SINTER, SUNION). Sorted Sets associate a score with each element and maintain sorted order — use for leaderboards (ZADD, ZRANGE, ZRANK), priority queues, rate limiting with sliding windows (ZADD with timestamp as score, ZREMRANGEBYSCORE to remove old entries, ZCARD to count), and time-series data. Streams (added in Redis 5.0) provide an append-only log with consumer groups — use for event streaming, audit logs, and pub/sub with persistence and consumer acknowledgment. Pub/Sub provides fire-and-forget messaging between publishers and subscribers — use for real-time notifications, but messages are lost if no subscriber is listening (use Streams for durable messaging). HyperLogLog provides probabilistic cardinality counting using only 12 KB per key — use for counting unique events (page views, unique users) at massive scale with less than 1% error. Bitmaps enable bitwise operations — use for feature flags, daily active user tracking, and bloom-filter-like structures. In .NET, use StackExchange.Redis as the client library with ConnectionMultiplexer for connection pooling.",
                Difficulty: "Senior",
                Tags: ["redis", "data-structures", "caching", "sorted-sets", "streams"]),
            new(
                Question: "Describe database migration strategies for production systems with zero downtime.",
                ExpectedAnswer: "Zero-downtime migrations require careful planning because you cannot lock the database while millions of users are active. The expand-and-contract pattern is the foundation: first expand (add new columns, tables, or indexes without removing anything), deploy new application code that writes to both old and new structures, backfill historical data, switch reads to the new structure, then contract (remove the old structure in a subsequent release). For column renames: add the new column, deploy code that writes to both columns, backfill old rows, switch reads to the new column, then drop the old column in a later release. For adding a NOT NULL column: add it as nullable first, backfill a default value in batches (not one giant UPDATE that locks the table), then add the NOT NULL constraint. For large table schema changes, use tools like pt-online-schema-change (MySQL), pg_repack (PostgreSQL), or perform online index operations in SQL Server (CREATE INDEX WITH (ONLINE = ON)). In EF Core, manage migrations with dotnet ef migrations add and apply them with dotnet ef database update or generate SQL scripts with dotnet ef migrations script for review before applying. For production, always generate and review the SQL script rather than running migrations directly. Use idempotent scripts (ef migrations script --idempotent) so they can be safely re-run. Separate data migrations from schema migrations — schema changes in the deployment pipeline, data backfills as background jobs. Always have a rollback plan: if the migration adds a column, the rollback drops it. Test migrations against a production-sized copy of the database to validate performance. Never run long-running data transformations inside a transaction — process in batches of 1000-10000 rows with commits between batches.",
                Difficulty: "Staff",
                Tags: ["migrations", "zero-downtime", "expand-contract", "schema-evolution"]),
            new(
                Question: "Explain the Write-Ahead Log (WAL) and its role in database crash recovery.",
                ExpectedAnswer: "The Write-Ahead Log is a sequential log file where the database records all changes before applying them to the actual data pages. The WAL protocol has one fundamental rule: a data page cannot be flushed to disk until all log records describing changes to that page have been flushed to the WAL first. This guarantees durability: if the system crashes after writing to the WAL but before updating data pages, the database can replay the WAL on recovery to reconstruct the committed changes (redo). If a transaction was not committed, its changes can be rolled back using the WAL's undo information. In PostgreSQL, the WAL is a series of 16 MB segment files in the pg_wal directory. WAL records contain enough information to redo the operation (the after-image) and, for some record types, undo it. Checkpoints periodically flush all dirty data pages to disk and record the checkpoint position in the WAL — during recovery, the database only needs to replay WAL records after the last checkpoint, reducing recovery time. The WAL also enables replication: streaming replication sends WAL records to replica servers that apply them to stay in sync with the primary. In SQL Server, the equivalent is the transaction log (.ldf file), which records all modifications. The recovery process has three phases: Analysis (determine which transactions were active at crash time), Redo (replay all logged operations from the last checkpoint to bring data pages up to date), and Undo (roll back all transactions that were not committed). WAL-based logging is also why sequential write performance of the log disk is critical for database throughput — use fast SSDs or dedicated disks for the WAL.",
                Difficulty: "Senior",
                Tags: ["wal", "crash-recovery", "durability", "replication", "transaction-log"]),
            new(
                Question: "What is connection pooling and why is it critical for database performance in .NET applications?",
                ExpectedAnswer: "Connection pooling maintains a cache of open database connections that are reused across requests instead of opening and closing connections for each operation. Establishing a TCP connection to a database involves DNS resolution, TCP handshake, TLS negotiation, and authentication — this can take 50-200ms. With pooling, a connection is borrowed from the pool, used, and returned (not closed) in sub-millisecond time. In ADO.NET, connection pooling is automatic and managed per unique connection string. When you call connection.Open(), the pool provides an existing idle connection or creates a new one if none is available and the pool has not reached its maximum size. When you call connection.Dispose() or connection.Close(), the connection is returned to the pool, not actually closed. Key configuration parameters in the connection string: Min Pool Size (connections pre-created at startup, default 0), Max Pool Size (maximum concurrent connections, default 100 in SQL Server), Connection Lifetime (maximum age of a connection before it is recycled, useful for load-balanced database clusters to redistribute connections), and Connection Timeout (how long to wait for a pool connection before throwing). Common problems: pool exhaustion occurs when all connections are in use and new requests wait until timeout — this is almost always caused by connection leaks where connections are not disposed properly. Always use 'using' statements or 'await using' for connections. Monitor pool usage with performance counters or events. In EF Core, the DbContext manages connection lifetime — AddDbContext uses scoped lifetime by default, and the connection is returned to the pool when the DbContext is disposed at the end of the request. Npgsql offers advanced pooling with multiplexing mode, which sends commands from multiple callers over a single TCP connection, reducing connection count for high-concurrency scenarios.",
                Difficulty: "Mid",
                Tags: ["connection-pooling", "ado-net", "performance", "npgsql"]),
        ],
        KeyConcepts:
        [
            "B-tree indexes and index internals",
            "Query optimizer and execution plans",
            "ACID properties",
            "Write-Ahead Log (WAL)",
            "Multi-Version Concurrency Control (MVCC)",
            "Partitioning vs Sharding",
            "Connection pooling",
            "Isolation levels and locking",
            "Document database modeling",
            "Redis data structures",
        ],
        Resources:
        [
            new("SQL Server Execution Plans by Grant Fritchey", "Book", "https://www.red-gate.com/simple-talk/books/sql-server-execution-plans/"),
            new("Use The Index, Luke", "Article", "https://use-the-index-luke.com/"),
            new("PostgreSQL Documentation", "Documentation", "https://www.postgresql.org/docs/current/"),
            new("Redis University", "Practice", "https://university.redis.com/"),
        ]);
    // ───────────────────────────────────────────────
    // Topic 6: ORM & Data Access
    // ───────────────────────────────────────────────
    private static TopicArea BuildOrmDataAccess() => new(
        Id: "orm-data-access",
        Name: "ORM & Data Access",
        Description: "Entity Framework Core, Dapper, and data access patterns including change tracking, query optimization, concurrency handling, and the trade-offs between different data access strategies in .NET applications.",
        Questions:
        [
            new(
                Question: "How does EF Core change tracking work? What are the different entity states?",
                ExpectedAnswer: "EF Core's change tracker monitors entities loaded into the DbContext and detects modifications when SaveChanges() is called. Each tracked entity has an EntityState: Added (new entity, will be INSERTed), Unchanged (loaded from database, no modifications detected), Modified (one or more properties changed, will be UPDATEd), Deleted (marked for removal, will be DELETEd), and Detached (not tracked by the context). When you query entities (e.g., context.Orders.Where(...).ToList()), EF Core stores a snapshot of their original property values in the change tracker. When SaveChanges() is called, the DetectChanges() method compares current property values against the snapshots to identify modifications. For each Modified entity, EF Core generates an UPDATE statement that includes only the changed columns, not all columns. You can also use change-tracking proxies (lazy loading proxies that intercept property setters) or implement INotifyPropertyChanged for notification-based tracking, which avoids the snapshot comparison overhead. The change tracker also manages navigation property fixup — when you load related entities, it automatically connects them via navigation properties. Performance considerations: tracking many entities consumes memory for the snapshots. For read-only queries, use AsNoTracking() to skip change tracking entirely, reducing memory usage and improving query performance by 10-30%. AsNoTrackingWithIdentityResolution() skips snapshots but still deduplicates entities by primary key, useful when a query returns the same entity multiple times through different joins.",
                Difficulty: "Mid",
                Tags: ["ef-core", "change-tracking", "entity-state", "as-no-tracking"]),
            new(
                Question: "Explain lazy loading, eager loading, and explicit loading in EF Core. What are the trade-offs?",
                ExpectedAnswer: "Eager loading uses .Include() and .ThenInclude() to load related entities in the same query via JOINs or additional queries: context.Orders.Include(o => o.OrderLines).ThenInclude(ol => ol.Product). The data comes back in one round trip (or a few if query splitting is used), ensuring no surprise queries later. The trade-off is that you might load more data than needed, and JOINs on multiple collections can produce a cartesian explosion where the result set multiplies in size. Lazy loading defers loading of navigation properties until they are accessed. EF Core supports it via proxies (UseLazyLoadingProxies()) which requires virtual navigation properties, or via ILazyLoader injection. When you access order.OrderLines for the first time, a separate query is executed transparently. The danger is the N+1 problem: iterating over 100 orders and accessing each order's OrderLines triggers 100 additional queries (1 for orders + 100 for their lines). This is a severe performance problem in loops. Lazy loading should be avoided in most server-side applications. Explicit loading gives you manual control: context.Entry(order).Collection(o => o.OrderLines).Load() or .Query().Where(...).Load() to filter related entities before loading. This is useful when you conditionally need related data or want to apply filters. My recommendation is to use eager loading by default with Include() for known data requirements, use explicit loading when related data is conditionally needed, and avoid lazy loading entirely in web applications. Use AsSplitQuery() when Include() on multiple collections produces cartesian explosion — it splits into separate SQL queries per collection but maintains a single round trip.",
                Difficulty: "Mid",
                Tags: ["eager-loading", "lazy-loading", "explicit-loading", "n-plus-one"]),
            new(
                Question: "What are compiled queries in EF Core and when should you use them?",
                ExpectedAnswer: "Compiled queries pre-compile the LINQ expression tree to SQL once and cache the result, avoiding the overhead of expression tree traversal, SQL generation, and parameter extraction on every execution. In EF Core, use EF.CompileQuery or EF.CompileAsyncQuery: private static readonly Func<AppDbContext, int, Task<Order?>> GetOrderById = EF.CompileAsyncQuery((AppDbContext ctx, int id) => ctx.Orders.Include(o => o.OrderLines).FirstOrDefault(o => o.Id == id)). Then call: var order = await GetOrderById(context, orderId). The performance benefit is most significant for simple queries that are executed frequently (thousands of times per second) because it eliminates the LINQ translation overhead, which can be 1-5 microseconds per query. For complex queries that take milliseconds to execute against the database, the translation overhead is negligible in comparison. EF Core also has an internal query cache that caches previously translated queries based on the expression tree shape, so even without explicit CompileQuery, repeated identical LINQ patterns hit the cache. However, the cache has a size limit and uses expression tree comparison which is not free. CompileQuery guarantees zero translation overhead. Use compiled queries for hot-path queries in high-throughput services where you have profiling data showing LINQ translation as a bottleneck. Do not prematurely optimize — measure with BenchmarkDotNet first. One limitation is that compiled queries cannot use dynamic filtering (conditionally appending Where clauses) because the expression tree shape must be fixed at compile time.",
                Difficulty: "Senior",
                Tags: ["compiled-queries", "ef-core", "performance", "query-cache"]),
            new(
                Question: "When would you use raw SQL with EF Core, and what are the options?",
                ExpectedAnswer: "EF Core provides several ways to execute raw SQL. FromSqlRaw and FromSqlInterpolated execute SQL that maps to an entity type: context.Orders.FromSqlInterpolated($\"SELECT * FROM Orders WHERE CustomerId = {customerId}\").Include(o => o.OrderLines).Where(o => o.Total > 100). The SQL result must map to the entity type's columns, and you can chain LINQ operators after FromSql which EF Core wraps in a subquery. SqlQueryRaw and SqlQueryInterpolated (EF Core 7+) execute SQL that returns arbitrary result sets mapped to scalar types or unmapped types. ExecuteSqlInterpolated executes non-query commands (INSERT, UPDATE, DELETE) and returns the affected row count: context.Database.ExecuteSqlInterpolated($\"UPDATE Orders SET Status = {newStatus} WHERE Id = {id}\"). Always use the interpolated variants rather than Raw — the interpolated versions automatically parameterize the interpolated values, preventing SQL injection. The Raw variants require you to manage DbParameter objects manually. Use raw SQL when: LINQ cannot express the query efficiently (complex window functions, recursive CTEs, database-specific features), when you need to call stored procedures, when you have performance-critical queries where handwritten SQL is measurably faster, or when migrating from existing SQL queries and rewriting in LINQ is not worth the effort. Even with raw SQL, EF Core's change tracking still works for tracked entities returned from FromSql. For truly complex scenarios where you need full control, use ADO.NET directly via context.Database.GetDbConnection() or switch to Dapper for that specific query while keeping EF Core for the rest of the application.",
                Difficulty: "Mid",
                Tags: ["raw-sql", "ef-core", "sql-injection", "from-sql"]),
            new(
                Question: "Compare Dapper and EF Core for performance and developer productivity. When would you use each?",
                ExpectedAnswer: "Dapper is a micro-ORM that maps SQL query results directly to objects. It adds minimal overhead on top of raw ADO.NET — typically less than 5% — making it nearly as fast as hand-written data access code. Dapper gives you full control over the SQL, which means you can write perfectly optimized queries, use database-specific features, and avoid the abstraction cost of LINQ translation. However, Dapper requires writing SQL by hand, provides no change tracking, no migrations, no lazy loading, and no unit of work — you manage connections, transactions, and INSERT/UPDATE/DELETE statements yourself. EF Core is a full-featured ORM with LINQ-based querying, automatic change tracking, migration generation, relationship management, and database-agnostic abstraction. It reduces boilerplate significantly but adds overhead: LINQ expression tree translation, change tracking memory, and occasionally generates suboptimal SQL for complex queries. EF Core's performance has improved dramatically in recent versions — for most CRUD operations, the difference from Dapper is 10-30%, which is rarely the bottleneck compared to actual database I/O. Use EF Core as your primary data access layer for its productivity benefits: LINQ queries are refactorable, compile-time checked, and migrations handle schema evolution. Use Dapper for specific hot-path queries where benchmarking shows EF Core overhead matters, for complex reporting queries with CTEs and window functions that are easier to express in SQL, and for bulk operations. The two can coexist in the same application: use EF Core for general CRUD and domain operations, and Dapper for performance-critical reads and complex reports, both sharing the same connection string and even the same DbConnection.",
                Difficulty: "Mid",
                Tags: ["dapper", "ef-core", "micro-orm", "performance-comparison"]),
            new(
                Question: "Discuss the Repository pattern with EF Core: is it necessary or an unnecessary abstraction?",
                ExpectedAnswer: "The Repository pattern abstracts data access behind an interface (IOrderRepository with methods like GetByIdAsync, AddAsync, etc.), while DbContext already implements the Repository pattern (DbSet<T>) and Unit of Work pattern (SaveChanges). Adding another Repository layer on top is controversial. Arguments for: it provides a seam for unit testing (mock IOrderRepository instead of DbContext), encapsulates query logic in one place so the same complex query is not duplicated across services, and allows swapping the data access technology (though this rarely happens in practice). Arguments against: it often becomes a leaky abstraction — you need IQueryable to avoid the N+1 problem and to allow callers to compose filters, but exposing IQueryable means callers depend on the ORM's capabilities, defeating the abstraction. Generic repositories (IRepository<T> with GetAll, GetById, Add, Remove) are especially problematic because they provide a thin, often unhelpful wrapper that prevents using EF Core features like Include(), AsNoTracking(), and query filters. A better alternative is the CQRS approach: for commands, use DbContext directly in command handlers (the handler is the 'repository' for that operation), and for queries, use dedicated query services or read-only repositories that return DTOs via projection. Specification pattern (using libraries like Ardalis.Specification) is another option that encapsulates query criteria and includes into reusable objects without wrapping DbContext in a generic repository. My recommendation: skip the generic repository, use DbContext directly in handlers, extract reusable query logic into extension methods on IQueryable<T> or specification objects, and mock at the database level using EF Core's in-memory provider or SQLite for integration tests.",
                Difficulty: "Senior",
                Tags: ["repository-pattern", "ef-core", "abstraction", "unit-of-work"]),
            new(
                Question: "How do you handle optimistic concurrency in EF Core?",
                ExpectedAnswer: "Optimistic concurrency assumes conflicts are rare and detects them at save time rather than locking rows during reads. In EF Core, configure a concurrency token — a property whose value is checked in the WHERE clause of UPDATE and DELETE statements. If the value in the database has changed since the entity was loaded, zero rows are affected and EF Core throws DbUpdateConcurrencyException. There are two approaches. A row version / timestamp column: in SQL Server, use [Timestamp] attribute or modelBuilder.Entity<Order>().Property(o => o.RowVersion).IsRowVersion(). The database automatically updates this byte[] value on every row modification. In PostgreSQL, use the xmin system column as a concurrency token via UseXminAsConcurrencyToken(). A regular property as a concurrency token: mark any property with [ConcurrencyCheck] — EF Core includes its original value in the WHERE clause. Handling the exception involves catching DbUpdateConcurrencyException, loading the current database values with entry.GetDatabaseValues(), comparing them with the original and current values, and deciding the resolution strategy: client wins (overwrite database values), database wins (discard the user's changes), or merge (apply non-conflicting changes, prompt the user for conflicting ones). For a web API, return 409 Conflict with the current version so the client can merge and retry. Include the RowVersion in your API response (as a base64 string or ETag header) and require it in update requests. This pattern is preferred over pessimistic locking in web applications because HTTP requests should be stateless and short-lived — holding database locks across user think time causes contention and deadlocks.",
                Difficulty: "Senior",
                Tags: ["optimistic-concurrency", "ef-core", "row-version", "conflict-resolution"]),
            new(
                Question: "What is the N+1 query problem and how do you detect and fix it in EF Core?",
                ExpectedAnswer: "The N+1 problem occurs when code loads a collection of N parent entities and then individually loads related data for each one, resulting in 1 query for the parents plus N queries for the children. For example, loading 50 orders and then accessing order.Customer for each one in a loop triggers 50 additional queries if Customer is lazy-loaded or not eagerly included. Detection: enable EF Core's query logging by configuring optionsBuilder.LogTo(Console.WriteLine, LogLevel.Information) or use a diagnostic tool like MiniProfiler or EF Core's query tags. In development, enable optionsBuilder.EnableSensitiveDataLogging() and optionsBuilder.EnableDetailedErrors(). Look for patterns of repeated similar queries in the log output. Fixes: use eager loading with .Include(o => o.Customer) to load related data in the same query via a JOIN. Use .Select() projection to load only the fields you need into a DTO, which also avoids loading unnecessary columns: context.Orders.Select(o => new OrderDto { Id = o.Id, CustomerName = o.Customer.Name }). EF Core translates navigation property access in Select projections into JOINs automatically. For batch loading scenarios, load all parent IDs first, then load all children in a single WHERE IN query: var customerIds = orders.Select(o => o.CustomerId).Distinct(); var customers = context.Customers.Where(c => customerIds.Contains(c.Id)).ToDictionary(c => c.Id). Use AsSplitQuery() when multiple Include() calls cause cartesian explosion (multiple collection navigations producing a cross join). Split queries execute separate SQL queries per Include but avoid the data duplication of cartesian products.",
                Difficulty: "Mid",
                Tags: ["n-plus-one", "ef-core", "eager-loading", "projection"]),
            new(
                Question: "How do global query filters work in EF Core and what are their limitations?",
                ExpectedAnswer: "Global query filters are LINQ predicates applied automatically to all queries for a given entity type, configured in OnModelCreating: modelBuilder.Entity<Order>().HasQueryFilter(o => !o.IsDeleted) for soft delete, or modelBuilder.Entity<TenantEntity>().HasQueryFilter(e => e.TenantId == _currentTenantId) for multi-tenancy. Every LINQ query, Include(), and navigation property access for that entity type will include the filter in the generated WHERE clause. This ensures soft-deleted rows or other-tenant data is never accidentally returned. You can bypass the filter for specific queries using .IgnoreQueryFilters(): context.Orders.IgnoreQueryFilters().Where(...). Limitations: only one filter per entity type (though you can combine conditions with && in a single filter expression). Filters cannot reference other navigation properties in complex ways — the filter expression must be translatable to SQL. When used for multi-tenancy, the filter accesses a field on the DbContext (like _currentTenantId), which means the DbContext must be scoped to the request and the tenant ID set before queries execute. This can cause issues with DbContext pooling (AddDbContextPool) because pooled contexts retain state — you must implement IDbContextFactory or reset the tenant ID when the context is leased from the pool. Filters apply to Include() as well, so if you soft-delete a child entity, it will not appear when loading the parent's collection. For navigation property access in filters, referencing another entity's property (e.g., filtering Post by Blog.IsActive) generates a JOIN in every query for that entity, which may have performance implications.",
                Difficulty: "Senior",
                Tags: ["global-query-filters", "ef-core", "soft-delete", "multi-tenancy"]),
            new(
                Question: "Explain query splitting in EF Core. When does a single query become problematic?",
                ExpectedAnswer: "When a LINQ query uses multiple Include() calls for collection navigations, EF Core generates a single SQL query with LEFT JOINs. If an Order has 10 OrderLines and 5 Shipments, the single query produces a cartesian product of 10 x 5 = 50 result rows, with the Order columns duplicated in every row. This is called cartesian explosion and can produce enormous result sets that transfer excessive data over the network, consume memory, and slow down materialization. Query splitting, introduced in EF Core 5, addresses this by executing separate SQL queries per collection Include: one query for Orders with OrderLines (10 rows) and another for Orders with Shipments (5 rows). Enable it per-query with .AsSplitQuery() or globally with optionsBuilder.UseQuerySplittingBehavior(QuerySplittingBehavior.SplitQuery). The trade-offs are significant. Split queries avoid cartesian explosion and reduce data transfer, but they introduce multiple database round trips and, critically, do not guarantee data consistency between the queries — if another transaction modifies data between the first and second query, you could get inconsistent results. Single queries guarantee a point-in-time snapshot because the entire result comes from one statement. Split queries can also lead to the N+1 problem for large parent collections if EF generates separate queries per parent rather than batching. My recommendation: default to single queries (AsSingleQuery), switch to split queries only when you observe cartesian explosion with multiple collection navigations, and be aware of the consistency trade-off. For read-heavy operations where snapshot consistency matters, prefer single queries with projection (.Select() to a DTO that flattens the data structure).",
                Difficulty: "Senior",
                Tags: ["query-splitting", "ef-core", "cartesian-explosion", "include"]),
        ],
        KeyConcepts:
        [
            "DbContext lifetime and scoping",
            "AsNoTracking for read-only queries",
            "IQueryable vs IEnumerable deferred execution",
            "N+1 query problem",
            "Global query filters",
            "Value converters and owned types",
            "Change tracking and entity states",
            "Compiled queries",
            "Optimistic concurrency with row version",
            "Query splitting vs single query",
        ],
        Resources:
        [
            new("EF Core Documentation", "Documentation", "https://learn.microsoft.com/en-us/ef/core/"),
            new("Entity Framework Core in Action by Jon P. Smith", "Book", "https://www.manning.com/books/entity-framework-core-in-action-second-edition"),
            new("Dapper Documentation", "Documentation", "https://github.com/DapperLib/Dapper"),
            new("EF Core Performance Tips", "Article", "https://learn.microsoft.com/en-us/ef/core/performance/"),
        ]);
    // ───────────────────────────────────────────────
    // Topic 7: Performance
    // ───────────────────────────────────────────────
    private static TopicArea BuildPerformance() => new(
        Id: "performance",
        Name: "Performance",
        Description: "Profiling, benchmarking, and optimization techniques for .NET applications including memory management, allocation reduction, caching strategies, and identifying and resolving performance bottlenecks.",
        Questions:
        [
            new(
                Question: "How would you use dotMemory or PerfView to diagnose a memory leak in a .NET application?",
                ExpectedAnswer: "A memory leak in .NET means objects that are no longer needed are still reachable from GC roots, preventing collection. With dotMemory, take two memory snapshots: one at baseline and another after the suspected leak has accumulated. Compare the snapshots to see which object types have grown in count and retained size. Drill into the dominator tree to see which root objects are holding references to the leaked objects. Common leak sources include event handlers not being unsubscribed (the event publisher holds a reference to the subscriber), static collections that grow indefinitely, closures capturing variables that keep large objects alive, and IDisposable objects not being disposed (particularly HttpClient instances, timers, and database connections). With PerfView, capture a GC heap dump using PerfView /GCCollectOnly collect or take a process dump and analyze it. Open the GC Heap Dump view, sort by Inclusive Size to find the largest object graphs, and use the 'Referred-From' view to trace why objects are rooted. PerfView's advantage is that it can analyze production dumps without installing tools on the server. For real-time monitoring, use dotnet-counters to watch GC heap size, Gen 0/1/2 collection rates, and LOH size. If GC heap grows continuously without stabilizing, you have a leak. If Gen 2 collections are frequent, long-lived objects are being promoted excessively. Another powerful technique is enabling EventPipe events and analyzing with dotnet-trace to see allocation stacks — which call sites are allocating the most memory. The key workflow is: monitor metrics to detect the symptom, take snapshots to identify the growing types, then trace references to find the root cause.",
                Difficulty: "Senior",
                Tags: ["memory-profiling", "dotmemory", "perfview", "memory-leak"]),
            new(
                Question: "Explain how to reduce allocations in hot paths using ArrayPool<T>, stackalloc, and object pooling.",
                ExpectedAnswer: "In high-throughput .NET applications, GC pressure from frequent allocations is a primary performance bottleneck. ArrayPool<T>.Shared.Rent(minimumLength) borrows a buffer from a shared pool, and Return(buffer) gives it back. The pool may return a larger array than requested, so always track the actual data length separately. Use it for temporary byte or char buffers in serialization, I/O, and string processing. Example: var buffer = ArrayPool<byte>.Shared.Rent(4096); try { int bytesRead = await stream.ReadAsync(buffer.AsMemory(0, 4096)); } finally { ArrayPool<byte>.Shared.Return(buffer); }. stackalloc allocates on the stack frame, which is freed automatically when the method returns — zero GC involvement. Combined with Span<T>: Span<byte> buffer = stackalloc byte[256]; Use it for small, fixed-size temporary buffers. The stack is limited (default 1 MB per thread), so only stackalloc small buffers; a common pattern is to stackalloc if the size is under a threshold and fall back to ArrayPool for larger sizes. ObjectPool<T> from Microsoft.Extensions.ObjectPool reuses heavyweight objects: StringBuilder, MemoryStream, or custom objects with expensive initialization. Register with services.AddSingleton(ObjectPool.Create<StringBuilder>()) and use: var sb = pool.Get(); try { sb.Append(...); } finally { pool.Return(sb); }. The pool's Return method calls a policy that resets the object (clears the StringBuilder). Other techniques: use string.Create() to build strings without intermediate allocations, use ReadOnlySpan<char> slicing instead of Substring(), use ValueTask<T> instead of Task<T> for methods that frequently complete synchronously, and use structs for small data transfer objects on hot paths to keep them on the stack.",
                Difficulty: "Senior",
                Tags: ["array-pool", "stackalloc", "object-pooling", "allocation-reduction"]),
            new(
                Question: "How do you set up proper benchmarks with BenchmarkDotNet? What common pitfalls should you avoid?",
                ExpectedAnswer: "BenchmarkDotNet is the standard .NET benchmarking library that handles warmup, statistical analysis, GC measurement, and JIT optimization effects. Create a benchmark class with [Benchmark] methods and run with BenchmarkRunner.Run<MyBenchmark>(). Always run benchmarks in Release mode — Debug mode disables optimizations and produces meaningless results. BenchmarkDotNet automatically handles JIT warmup (running the method enough times for tiered compilation to produce Tier 1 code), performs multiple iterations with statistical analysis (mean, median, standard deviation, confidence intervals), and reports memory allocation metrics with [MemoryDiagnoser]. Common pitfalls: benchmarking code that the JIT can eliminate entirely — if you compute a result but never use it, the JIT may optimize the entire computation away. Always return or consume the result, or use [Benchmark] methods that return the computed value. Avoid benchmarking in-process with other work running — BenchmarkDotNet spawns a separate process for isolation. Do not use DateTime.Now or Stopwatch for microbenchmarks — the resolution is insufficient and you are not accounting for warmup, GC pauses, or statistical variance. Do not allocate in the benchmark method setup that should be in [GlobalSetup]. Use [Params] to test across different input sizes and [Arguments] for specific test cases. Compare implementations using the [Benchmark(Baseline = true)] attribute on the reference implementation. Use [DisassemblyDiagnoser] to verify the JIT is generating the expected machine code. For realistic results, benchmark with production-representative data sizes and patterns. Export results to markdown or HTML for documentation and comparison across runs.",
                Difficulty: "Mid",
                Tags: ["benchmarkdotnet", "benchmarking", "performance-testing", "methodology"]),
            new(
                Question: "Describe caching strategies in .NET: in-memory, distributed, and output caching.",
                ExpectedAnswer: "In-memory caching with IMemoryCache (AddMemoryCache()) stores data in the application process memory. It is the fastest option with nanosecond access times but is limited to a single process — not shared across servers. Use it for frequently accessed, rarely changing reference data (configuration, lookup tables). Configure size limits, absolute expiration, and sliding expiration to prevent unbounded growth. Distributed caching with IDistributedCache provides a shared cache across multiple application instances. Redis is the most common implementation (AddStackExchangeRedisCache()). It stores serialized byte arrays, so there is serialization overhead. Use it for session state, user-specific data, and any data that must be consistent across a web farm. Typical access time is 1-5ms over the network. A two-level cache pattern combines both: check IMemoryCache first (L1), fall back to IDistributedCache (L2), then fall back to the database. Use IMemoryCache with a short TTL (30-60 seconds) backed by Redis with a longer TTL (5-15 minutes). Output caching (AddOutputCache() in .NET 7+) caches entire HTTP responses. Configure policies: builder.Services.AddOutputCache(options => { options.AddBasePolicy(builder => builder.Expire(TimeSpan.FromMinutes(5))); options.AddPolicy(\"ByUser\", builder => builder.VaryByHeader(\"Authorization\")); }). Output caching is ideal for public content served to many users. It replaces the older Response Caching middleware and is more flexible, supporting cache invalidation by tag. Cache invalidation is the hardest problem: use cache-aside pattern (read-through with explicit invalidation on writes), pub/sub notifications from Redis to invalidate L1 caches across servers, and always have a TTL as a safety net even with active invalidation.",
                Difficulty: "Mid",
                Tags: ["caching", "imemorycache", "redis", "output-caching", "distributed-cache"]),
            new(
                Question: "What are the common 'async all the way down' anti-patterns and why do they cause performance issues?",
                ExpectedAnswer: "The principle of 'async all the way down' means that if you call an async method, every method in the call chain should be async, from the entry point to the I/O operation. Common anti-patterns that violate this include: Sync-over-async — calling .Result or .Wait() on a Task, which blocks the calling thread while the async operation completes. On the thread pool, this wastes a thread that could be doing other work, and in ASP.NET Core can lead to thread pool starvation under load (all threads blocked waiting for I/O, no threads available to complete the I/O callbacks — a deadlock-like situation). In older ASP.NET with a SynchronizationContext, this caused actual deadlocks. Always use 'await' instead. Async-over-sync — wrapping a synchronous CPU-bound method in Task.Run() and awaiting it in an ASP.NET Core controller. This does not improve throughput — it just shifts the work from one thread pool thread to another, adding overhead. Task.Run is for desktop applications to keep the UI thread responsive. In ASP.NET Core, all code already runs on the thread pool, so Task.Run adds overhead without benefit. Only use async for genuinely asynchronous I/O operations. Missing ConfigureAwait(false) in library code — by default, await captures the SynchronizationContext and resumes on it. In library code, ConfigureAwait(false) avoids this capture, reducing overhead and preventing deadlocks when consumed by code with a SynchronizationContext. In ASP.NET Core there is no SynchronizationContext, so it is less critical but still a good practice in libraries. Async void — should only be used for event handlers. Async void methods cannot be awaited, exceptions cannot be caught by the caller, and they complicate testing. Use async Task instead.",
                Difficulty: "Senior",
                Tags: ["async-anti-patterns", "thread-pool-starvation", "sync-over-async", "configure-await"]),
            new(
                Question: "How would you diagnose and optimize high CPU usage in a .NET application?",
                ExpectedAnswer: "Start by identifying whether the CPU usage is from user code or the GC. Use dotnet-counters to monitor '% Time in GC' — if it is above 20%, the bottleneck is GC pressure from excessive allocations, not your application logic. For user-code CPU issues, capture a CPU profile using dotnet-trace collect --profile cpu-sampling --process-id PID or PerfView's CPU Stacks view. The profile shows a call tree with inclusive and exclusive samples per method. Exclusive samples (time spent in the method body itself, not callees) identify the actual hot methods. Sort by exclusive samples to find the top CPU consumers. Common causes include: inefficient algorithms with high time complexity (O(n^2) loops that should be O(n log n)), excessive string concatenation (use StringBuilder), repeated LINQ queries that enumerate the same collection multiple times (materialize with ToList()), unnecessary serialization/deserialization, regex without Compiled or source-generated options on hot paths, and reflection-heavy code (use source generators or cached delegates). For GC-caused CPU: profile allocations with dotnet-trace using the gc-collect profile, identify which methods allocate the most (allocation stacks), and reduce allocations using Span<T>, ArrayPool, stackalloc, value types, and string.Create(). Use BenchmarkDotNet with [MemoryDiagnoser] to measure allocations of candidate optimizations. In production, use Application Insights or OpenTelemetry to identify slow endpoints, then drill into specific hot methods with targeted profiling. The optimization workflow is: measure, identify the bottleneck, form a hypothesis, implement the fix, and benchmark to verify the improvement. Never optimize without profiling data.",
                Difficulty: "Senior",
                Tags: ["cpu-profiling", "dotnet-trace", "perfview", "optimization"]),
            new(
                Question: "Explain Gen 0, Gen 1, and Gen 2 GC pressure. How does LOH fragmentation occur and how do you mitigate it?",
                ExpectedAnswer: "Gen 0 pressure means frequent Gen 0 collections due to high allocation rates. Gen 0 is collected when its budget (typically a few MB) is exhausted. Short-lived temporary objects (strings, LINQ intermediate collections, lambda closures, boxing) cause Gen 0 pressure. High Gen 0 collection rates (hundreds per second) indicate too many allocations. Gen 1 pressure occurs when objects survive Gen 0 — often mid-lifetime objects like cache entries with short TTLs or objects referenced just long enough to be promoted. Gen 1 acts as a filter; excessive Gen 1 collections suggest objects are living slightly too long. Gen 2 pressure is the most expensive: Gen 2 collections pause the application longer, scan the entire managed heap, and trigger LOH collection. Frequent Gen 2 collections indicate too many long-lived allocations or memory leaks. The Large Object Heap stores objects 85,000+ bytes (primarily large arrays and strings). Unlike Gen 0-2, the LOH is not compacted by default. When large objects are allocated and freed repeatedly, they leave gaps in the LOH. New large allocations may not fit in existing gaps, requiring new segments, while the gaps waste memory — this is LOH fragmentation. Mitigation strategies: reuse large arrays with ArrayPool<T>.Shared (the most effective approach), avoid allocating arrays slightly above 85,000 bytes (consider two smaller arrays), use RecyclableMemoryStream from Microsoft.IO.RecyclableMemoryStream for large MemoryStream operations, and in extreme cases enable LOH compaction with GCSettings.LargeObjectHeapCompactionMode = GCLargeObjectHeapCompactionMode.CompactOnce (compacts on the next Gen 2 GC, then resets — use sparingly as compaction is expensive). The Pinned Object Heap (POH, .NET 5+) segregates pinned objects away from the regular SOH to prevent them from causing fragmentation there.",
                Difficulty: "Senior",
                Tags: ["gc-pressure", "loh-fragmentation", "gen0", "gen2", "array-pool"]),
            new(
                Question: "What is the System.IO.Pipelines API and when would you use it over traditional Stream-based I/O?",
                ExpectedAnswer: "System.IO.Pipelines (the Pipe, PipeReader, PipeWriter API) is a high-performance I/O library designed for protocol parsing scenarios like network servers. Traditional Stream-based I/O has several problems: you must allocate and manage byte[] buffers, handle partial reads (a single Read may not return a complete message), manage buffer resizing when messages span multiple reads, and copy data between buffers. Pipelines solves these problems with a producer-consumer model. The writer (PipeWriter) writes data into the pipe's internal buffer pool (which uses ArrayPool under the hood), and the reader (PipeReader) reads sequences of bytes as ReadOnlySequence<byte>, which may span multiple buffer segments without copying. The reader advances through data, marking consumed bytes (which can be recycled) and examined bytes (not yet processed). This eliminates buffer management and copying. Key benefits: back-pressure is built in — if the reader falls behind, the writer naturally pauses when the pipe buffer is full. Memory management is handled by the pipe — no manual ArrayPool rent/return. ReadOnlySequence<byte> represents potentially discontiguous memory, allowing efficient handling of data that spans multiple buffers without concatenation. Kestrel, ASP.NET Core's web server, uses Pipelines internally for HTTP request parsing, which is a major reason for its high performance. Use Pipelines when building custom network protocols (TCP servers, WebSocket handlers), implementing high-throughput data processing pipelines, or any scenario where you are reading a stream of framed messages. For simple file or HTTP operations, Stream-based I/O is perfectly adequate. Pipelines is specifically optimized for scenarios where you need to parse structured data from a continuous byte stream with minimal allocations.",
                Difficulty: "Staff",
                Tags: ["pipelines", "system-io-pipelines", "high-performance-io", "kestrel"]),
            new(
                Question: "How does response compression work in ASP.NET Core and what are the considerations?",
                ExpectedAnswer: "Response compression reduces the size of HTTP response bodies using algorithms like Gzip or Brotli. In ASP.NET Core, add builder.Services.AddResponseCompression(options => { options.EnableForHttps = true; options.Providers.Add<BrotliCompressionProvider>(); options.Providers.Add<GzipCompressionProvider>(); }) and app.UseResponseCompression() before middleware that generates responses. The middleware checks the request's Accept-Encoding header, selects the best matching provider, and compresses the response body on the fly. Brotli typically achieves 15-25% better compression ratios than Gzip for text content, but is slower to compress. Configure compression levels: services.Configure<BrotliCompressionProviderOptions>(options => options.Level = CompressionLevel.Fastest) for a good balance. Important considerations: HTTPS compression is disabled by default because of the BREACH/CRIME security attacks — an attacker who can inject content into the response and observe the compressed size can extract secrets like CSRF tokens. Enable it only if you understand the risk and mitigate it (e.g., by using per-request CSRF tokens). Do not compress already-compressed content (images, videos, zip files) — it wastes CPU without reducing size. Set Content-Type MIME type filters to compress only text-based content types (application/json, text/html, text/css, application/javascript). For static files, use pre-compressed files (Brotli and Gzip versions of .js and .css files generated at build time) served by UseStaticFiles — this avoids runtime compression overhead entirely. In production behind a reverse proxy like nginx or a CDN, compression is usually better handled at the proxy layer where it can cache compressed versions, offloading the work from the application server.",
                Difficulty: "Mid",
                Tags: ["response-compression", "brotli", "gzip", "asp-net-core"]),
            new(
                Question: "Describe database query optimization techniques from the application side in a .NET backend.",
                ExpectedAnswer: "Application-side database optimization starts with understanding access patterns and reducing unnecessary database work. Projection: always select only the columns you need. In EF Core, use .Select(x => new Dto { ... }) instead of loading full entities — this generates a targeted SELECT, reduces data transfer, and avoids change tracking overhead. Pagination: never load unbounded result sets. Use .Skip(offset).Take(pageSize) with keyset pagination for large datasets (WHERE Id > @lastId ORDER BY Id LIMIT @pageSize) to avoid the performance cliff of high OFFSET values. Batching: combine multiple database operations into a single round trip. EF Core 7+ supports ExecuteUpdate and ExecuteDelete for bulk operations without loading entities. Use AddRange() and SaveChanges() to batch inserts. For Dapper, use multi-mapping queries and multi-result queries to fetch related data in a single call. Connection management: ensure connections are returned to the pool promptly — avoid holding connections during external HTTP calls or user interactions. Use async database operations (ToListAsync, ExecuteAsync) to free the thread pool thread while waiting for the database. Caching: cache frequently accessed, rarely changing data. Use IMemoryCache for reference data with short TTL, IDistributedCache for shared state, and implement cache-aside with proper invalidation. Avoid N+1 queries: use Include() for eager loading or projection to flatten results. Index alignment: ensure your WHERE clauses, ORDER BY clauses, and JOIN conditions align with existing indexes. Use EF Core's query logging to inspect generated SQL and verify it uses seeks rather than scans. Connection string tuning: set appropriate Command Timeout, enable connection pooling with optimal Min Pool Size and Max Pool Size, and use Multiple Active Result Sets (MARS) only if needed (it adds overhead).",
                Difficulty: "Mid",
                Tags: ["query-optimization", "pagination", "projection", "batching"]),
        ],
        KeyConcepts:
        [
            "Gen 0/Gen 1/Gen 2 GC pressure",
            "LOH fragmentation and mitigation",
            "Span-based parsing and slicing",
            "System.IO.Pipelines API",
            "Memory-mapped files",
            "Hot path optimization",
            "ArrayPool and object pooling",
            "BenchmarkDotNet methodology",
            "Async anti-patterns",
            "Cache-aside and multi-level caching",
        ],
        Resources:
        [
            new("Writing High-Performance .NET Code by Ben Watson", "Book", "https://www.writinghighperf.net/"),
            new("BenchmarkDotNet Documentation", "Documentation", "https://benchmarkdotnet.org/"),
            new("ASP.NET Core Performance Best Practices", "Documentation", "https://learn.microsoft.com/en-us/aspnet/core/performance/performance-best-practices"),
            new("Adam Sitnik — High Performance .NET", "Video", "https://www.youtube.com/results?search_query=adam+sitnik+high+performance+.net"),
        ]);

    // ───────────────────────────────────────────────
    // Topic 8: Concurrency & Async
    // ───────────────────────────────────────────────
    private static TopicArea BuildConcurrency() => new(
        Id: "concurrency",
        Name: "Concurrency & Async Programming",
        Description: "Async/await internals, parallel programming, thread synchronization, and concurrent data structures in .NET.",
        Questions:
        [
            new(
                Question: "How does async/await work internally in C#? Describe the state machine the compiler generates.",
                ExpectedAnswer: "When you mark a method as async, the compiler transforms it into a state machine struct implementing IAsyncStateMachine. Each await point becomes a state. The MoveNext method contains a switch on the current state. When the method hits an await on an incomplete task, it captures the current state number, stores locals into fields of the state machine struct, hooks up a continuation via the AsyncTaskMethodBuilder, and returns. When the awaited task completes, the continuation calls MoveNext again, the switch jumps to the next state, and execution resumes. The state machine is initially a struct on the stack, but if the first await doesn't complete synchronously, it gets boxed to the heap. ExecutionContext (including AsyncLocal values) is captured and flowed across awaits automatically. The SynchronizationContext or TaskScheduler determines where the continuation runs — in ASP.NET Core there is no SynchronizationContext so continuations run on the thread pool.",
                Difficulty: "Senior",
                Tags: ["async", "state-machine", "compiler"]),
            new(
                Question: "What is the difference between Task.Run, Task.Factory.StartNew, and just calling an async method?",
                ExpectedAnswer: "Calling an async method directly starts executing synchronously on the current thread until the first incomplete await — no thread pool involvement until that point. Task.Run queues the delegate to the thread pool immediately and returns a Task, which is ideal for offloading CPU-bound work from a UI or request thread. Task.Factory.StartNew is the older, lower-level API that gives more control (TaskCreationOptions, custom TaskScheduler) but has a dangerous pitfall: if you pass an async delegate, it returns Task<Task> and you must call Unwrap(). Task.Run handles async delegates correctly by unwrapping automatically. In ASP.NET Core, you should generally avoid Task.Run because requests are already on thread pool threads, so you'd just be adding overhead by hopping threads. Use Task.Run primarily in UI applications to keep the UI responsive or in console apps to parallelize CPU work.",
                Difficulty: "Mid",
                Tags: ["task", "threadpool", "async"]),
            new(
                Question: "Explain the difference between Semaphore, SemaphoreSlim, Mutex, and lock. When would you use each?",
                ExpectedAnswer: "lock (Monitor.Enter/Exit) is the simplest — it provides mutual exclusion within a single process, is reentrant, and has very low overhead since it uses a thin lock that spins briefly before falling back to a kernel wait. SemaphoreSlim is a lightweight in-process semaphore that supports limiting concurrency to N (not just 1 like lock), supports async waiting via WaitAsync, and does not use a kernel semaphore unless contention occurs. Semaphore is a kernel-level semaphore that works across processes but is heavier — use it only for cross-process synchronization. Mutex is a kernel-level mutual exclusion primitive that also works cross-process, is reentrant (owned by a thread), and can be named for cross-process use. In practice: use lock for simple critical sections, SemaphoreSlim(1,1) when you need async-compatible mutual exclusion, SemaphoreSlim(N,N) for throttling concurrency (like limiting parallel HTTP calls), and named Mutex for ensuring single-instance applications.",
                Difficulty: "Senior",
                Tags: ["synchronization", "threading", "semaphore"]),
            new(
                Question: "What are ConcurrentDictionary's trade-offs and when should you NOT use it?",
                ExpectedAnswer: "ConcurrentDictionary uses fine-grained locking with lock striping — it splits the internal buckets into segments, each with its own lock, so multiple threads can read and write to different segments simultaneously. Reads are lock-free. However, it has several trade-offs: the GetOrAdd and AddOrUpdate methods that take a factory delegate may invoke the factory multiple times under contention (the losing call's result is discarded), so the factory must be side-effect free. The Count property is expensive because it must acquire all locks. Enumeration provides a snapshot but is not atomic across the entire dictionary. You should NOT use ConcurrentDictionary when you need atomic multi-key operations, when most access is single-threaded (regular Dictionary is faster), when you need ordered enumeration (use ConcurrentSkipListMap pattern), or when the factory in GetOrAdd involves expensive computation — in that case use Lazy<T> as the value type to ensure the computation runs exactly once.",
                Difficulty: "Senior",
                Tags: ["concurrent-collections", "threading", "dictionary"]),
            new(
                Question: "What is ConfigureAwait(false) and when should you use it in library code vs application code?",
                ExpectedAnswer: "ConfigureAwait(false) tells the async machinery not to capture and marshal back to the original SynchronizationContext or TaskScheduler when resuming after an await. By default, await captures the current context and posts the continuation back to it. In UI apps (WPF/WinForms), this means the continuation runs on the UI thread. Using ConfigureAwait(false) in library code is recommended because library code shouldn't assume it needs a specific context, and capturing/restoring context has overhead. It also prevents deadlocks in the classic scenario where UI code calls .Result or .Wait() on an async method — the continuation tries to marshal back to the UI thread which is blocked waiting, causing a deadlock. In ASP.NET Core, there is no SynchronizationContext, so ConfigureAwait(false) has no behavioral effect, but it's still a micro-optimization that skips the context check. In application-level code (UI event handlers, controllers), you typically want the default behavior to stay on the UI thread or request context.",
                Difficulty: "Mid",
                Tags: ["async", "configureawait", "synchronization-context"]),
            new(
                Question: "Explain Channels in .NET and how they compare to BlockingCollection for producer-consumer patterns.",
                ExpectedAnswer: "System.Threading.Channels provides a modern, high-performance, async-first producer-consumer API. You create bounded or unbounded channels via Channel.CreateBounded<T> or Channel.CreateUnbounded<T>. Producers write via channel.Writer.WriteAsync and consumers read via channel.Reader.ReadAllAsync (which returns IAsyncEnumerable). Key advantages over BlockingCollection: Channels are fully async — BlockingCollection.Take() blocks a thread while Channel.Reader.ReadAsync releases the thread. Channels support backpressure naturally with bounded channels — when full, WriteAsync awaits until space is available. Channels are significantly faster (lower allocation, lock-free algorithms internally using CAS operations). Channels support single-consumer optimized mode via SingleReader option. BlockingCollection still has a place when you need to consume from multiple collections simultaneously via BlockingCollection.TakeFromAny, or when working with legacy synchronous code. For new code in .NET 6+, prefer Channels for all producer-consumer scenarios.",
                Difficulty: "Senior",
                Tags: ["channels", "producer-consumer", "async"]),
            new(
                Question: "What is a deadlock in async code and how do you prevent it?",
                ExpectedAnswer: "The classic async deadlock occurs when synchronous code calls .Result or .Wait() on an async task while running on a thread that has a SynchronizationContext (UI thread or old ASP.NET). The async method's continuation is posted to the context, but the context's thread is blocked waiting for the result — deadlock. Prevention strategies: never block on async code — use async all the way up the call stack. If you absolutely must call async from sync (like in a constructor), use Task.Run to offload to a thread pool thread that has no SynchronizationContext. In library code, use ConfigureAwait(false). In ASP.NET Core, this specific deadlock doesn't occur because there's no SynchronizationContext, but blocking on async still wastes a thread pool thread and can cause thread pool starvation under load, which is a different kind of deadlock — all threads are blocked waiting for tasks that need a thread to complete. Prevent thread pool starvation by never blocking on async in request-processing code.",
                Difficulty: "Mid",
                Tags: ["deadlock", "async", "synchronization-context"]),
            new(
                Question: "Describe Parallel.ForEachAsync and when to use it vs Task.WhenAll with Select.",
                ExpectedAnswer: "Parallel.ForEachAsync (introduced in .NET 6) processes an IAsyncEnumerable or IEnumerable source with controlled parallelism. It takes a ParallelOptions parameter with MaxDegreeOfParallelism and CancellationToken. It's ideal for I/O-bound batch operations like processing a list of URLs or database records concurrently. The key advantage over the Task.WhenAll + Select pattern is built-in concurrency limiting — with Task.WhenAll(items.Select(async i => ...)) all tasks start immediately, potentially launching thousands of concurrent operations. You'd need a SemaphoreSlim to throttle. Parallel.ForEachAsync handles throttling internally and processes items as slots become available, maintaining steady throughput. It also handles exceptions better — if one iteration throws, it cancels remaining work and propagates an AggregateException. For CPU-bound parallelism, Parallel.ForEach (sync version) or PLINQ are better choices since they partition work across thread pool threads efficiently.",
                Difficulty: "Senior",
                Tags: ["parallel", "async", "concurrency-control"]),
        ],
        KeyConcepts:
        [
            "Async state machine compilation",
            "SynchronizationContext and TaskScheduler",
            "Lock-free algorithms and CAS",
            "Thread pool work stealing",
            "Producer-consumer patterns",
            "Async deadlock prevention",
            "Concurrent collections internals",
            "Structured concurrency",
        ],
        Resources:
        [
            new("Stephen Cleary's Async Best Practices", "Article", "https://learn.microsoft.com/en-us/archive/msdn-magazine/2013/march/async-await-best-practices-in-asynchronous-programming"),
            new("Concurrency in C# Cookbook by Stephen Cleary", "Book", "https://stephencleary.com/"),
            new("System.Threading.Channels Docs", "Documentation", "https://learn.microsoft.com/en-us/dotnet/core/extensions/channels"),
            new("Threading in .NET", "Documentation", "https://learn.microsoft.com/en-us/dotnet/standard/threading/"),
        ]);

    // ───────────────────────────────────────────────
    // Topic 9: Cloud (Azure / AWS)
    // ───────────────────────────────────────────────
    private static TopicArea BuildCloud() => new(
        Id: "cloud",
        Name: "Cloud Architecture (Azure / AWS)",
        Description: "Cloud-native patterns, Azure and AWS services relevant to .NET, infrastructure as code, and cloud cost optimization.",
        Questions:
        [
            new(
                Question: "Compare Azure App Service, Azure Container Apps, and AKS. When would you choose each for a .NET application?",
                ExpectedAnswer: "Azure App Service is a fully managed PaaS — ideal for straightforward web apps, APIs, and background WebJobs. You get auto-scaling, deployment slots, built-in authentication, and zero infrastructure management. Choose it for standard web workloads where you don't need container orchestration. Azure Container Apps is a serverless container platform built on Kubernetes and KEDA — it provides container flexibility without Kubernetes complexity. It supports scale-to-zero, event-driven scaling (Service Bus, Kafka triggers), Dapr integration for microservices sidecars, and revision-based traffic splitting. Choose it for microservices, event-driven architectures, or when you need containers but don't want to manage Kubernetes. AKS (Azure Kubernetes Service) gives you full Kubernetes with custom node pools, fine-grained networking (CNI, network policies), service mesh, and complete control over the cluster. Choose it when you need advanced networking, custom operators, GPU workloads, or have existing Kubernetes expertise and complex multi-service deployments that exceed what Container Apps offers.",
                Difficulty: "Senior",
                Tags: ["azure", "containers", "deployment"]),
            new(
                Question: "How does Azure Service Bus differ from Azure Event Hubs and Azure Storage Queues? When would you use each?",
                ExpectedAnswer: "Azure Storage Queues are the simplest — basic FIFO messaging with at-least-once delivery, 64KB message size, up to 7 days retention, and very low cost. Use them for simple task queuing where order isn't critical and you don't need advanced features. Azure Service Bus is an enterprise message broker supporting queues and publish-subscribe topics with subscriptions. It provides at-most-once delivery (PeekLock with message deduplication), sessions for ordered FIFO processing per session, dead-letter queues, scheduled delivery, transactions, and messages up to 256KB (100MB in premium). Use it for business-critical workflows, ordered processing, request-reply patterns, and enterprise integration. Azure Event Hubs is a distributed streaming platform (similar to Apache Kafka) designed for massive event ingestion — millions of events per second. It uses partitioned consumer groups, retains events for up to 90 days (or indefinitely with capture), and supports replay from any offset. Use it for telemetry, IoT ingestion, event sourcing event stores, or log aggregation where you need high throughput and consumer replay capability.",
                Difficulty: "Senior",
                Tags: ["azure", "messaging", "service-bus", "event-hubs"]),
            new(
                Question: "What is the Retry pattern and how would you implement it with Polly in a .NET application?",
                ExpectedAnswer: "The Retry pattern handles transient failures by automatically re-attempting failed operations. Transient faults include network timeouts, HTTP 429/503, temporary database unavailability, and momentary connection drops. With Polly (now integrated as Microsoft.Extensions.Http.Resilience in .NET 8+), you configure retry policies: simple retry (fixed delay), exponential backoff (delays increase — 1s, 2s, 4s, 8s), and exponential backoff with jitter (adds randomness to prevent thundering herd). Implementation: builder.AddStandardResilienceHandler() adds a sensible default pipeline, or customize via AddResilienceHandler with Retry, CircuitBreaker, Timeout, and RateLimiter strategies. Key considerations: only retry on transient errors (not 400 Bad Request), make operations idempotent so retries are safe, set a maximum retry count to avoid infinite loops, use circuit breaker to stop retrying when a service is clearly down (fail fast), and add a hedging strategy for latency-critical paths that sends parallel requests after a delay.",
                Difficulty: "Mid",
                Tags: ["resilience", "polly", "retry", "cloud-patterns"]),
            new(
                Question: "Explain Infrastructure as Code. Compare Terraform, Bicep, and Pulumi for Azure deployments.",
                ExpectedAnswer: "Infrastructure as Code (IaC) manages cloud infrastructure through declarative or imperative code files rather than manual portal clicks, enabling version control, code review, repeatable deployments, and drift detection. Terraform by HashiCorp uses HCL (HashiCorp Configuration Language), is cloud-agnostic supporting AWS/Azure/GCP/others, has a massive provider ecosystem, and uses state files to track deployed resources. Downsides: state management complexity, HCL is not a full programming language. Bicep is Azure-native, compiles down to ARM templates, has first-class Azure support with full day-zero resource coverage, no state file needed (Azure IS the state), and is simpler syntax than ARM JSON. Downside: Azure-only. Pulumi uses real programming languages (C#, TypeScript, Python, Go), so .NET developers can define infrastructure in C# with full IDE support, loops, conditionals, and type safety. It supports multiple clouds and uses state management similar to Terraform. For Azure-only shops, Bicep is the pragmatic choice. For multi-cloud or .NET teams wanting C# everywhere, Pulumi excels. For teams with existing Terraform expertise, staying with Terraform is reasonable.",
                Difficulty: "Senior",
                Tags: ["iac", "terraform", "bicep", "devops"]),
            new(
                Question: "How do you manage secrets in a cloud-native .NET application?",
                ExpectedAnswer: "Never store secrets in code, config files, or environment variables in plain text. The hierarchy of approaches: Azure Key Vault (or AWS Secrets Manager) is the primary secrets store — reference secrets by URI, use managed identity for authentication so no credentials are needed in the app itself. In ASP.NET Core, use Azure.Extensions.AspNetCore.Configuration.Secrets to load Key Vault secrets as configuration values at startup. For local development, use dotnet user-secrets which stores secrets in a per-user JSON file outside the project directory. For CI/CD, use pipeline secret variables (Azure DevOps secret variables, GitHub Actions secrets) injected as environment variables at deploy time. For Kubernetes, use External Secrets Operator to sync from Key Vault to Kubernetes Secrets. Never log secrets — use structured logging with destructuring that excludes sensitive fields. Rotate secrets regularly and use short-lived credentials (managed identity tokens, workload identity federation) instead of long-lived API keys wherever possible.",
                Difficulty: "Mid",
                Tags: ["security", "secrets", "key-vault", "managed-identity"]),
            new(
                Question: "What is the Strangler Fig pattern and how would you use it to migrate a monolith to microservices on Azure?",
                ExpectedAnswer: "The Strangler Fig pattern incrementally replaces a legacy monolith by routing specific features to new services while the monolith continues to handle everything else. Named after strangler fig trees that grow around a host tree and eventually replace it. Implementation on Azure: place an API gateway or reverse proxy (Azure API Management, YARP) in front of the monolith. Start by identifying a bounded context to extract — preferably one with clear boundaries and high change frequency. Build the new microservice, deploy it to Container Apps or AKS, and configure the gateway to route that feature's requests to the new service. The monolith still handles all other routes. Gradually extract more bounded contexts. Use the Anti-Corruption Layer pattern to translate between the monolith's data model and the new service's domain model. Share data through events (Service Bus) not direct database access. Key advantage: zero big-bang risk, you can roll back any individual migration, and the monolith shrinks incrementally until it can be decommissioned.",
                Difficulty: "Senior",
                Tags: ["migration", "microservices", "architecture-patterns"]),
            new(
                Question: "Explain Azure Managed Identity and how it eliminates credential management in .NET applications.",
                ExpectedAnswer: "Azure Managed Identity provides an automatically managed identity in Azure AD for Azure resources, eliminating the need to store credentials in code or config. System-assigned managed identity is tied to a specific resource (App Service, VM, Container App) — created and deleted with the resource. User-assigned managed identity is a standalone Azure resource that can be shared across multiple resources. In .NET, use DefaultAzureCredential from Azure.Identity, which automatically tries multiple authentication methods in order: environment variables, managed identity, Visual Studio, Azure CLI, etc. So the same code works locally (using your Azure CLI login) and in production (using managed identity) with zero config changes. Grant the identity access via Azure RBAC — for example, assign 'Key Vault Secrets User' role for Key Vault access, or 'Storage Blob Data Contributor' for blob storage. No connection strings with keys needed — just the resource URI. This eliminates an entire class of security vulnerabilities (leaked credentials) and operational burden (secret rotation).",
                Difficulty: "Mid",
                Tags: ["azure", "managed-identity", "security", "authentication"]),
            new(
                Question: "How would you design a cost-effective architecture for a .NET application on Azure that has variable traffic?",
                ExpectedAnswer: "Start with auto-scaling at every layer. Use Azure Container Apps or App Service with auto-scale rules based on HTTP traffic, CPU, or custom metrics — scale to zero during off-hours with Container Apps. For databases, use Azure SQL Serverless tier which auto-pauses after inactivity and auto-scales compute within a range, or Cosmos DB with autoscale RU/s. Use Azure Functions (consumption plan) for event-driven workloads that are spiky — you pay only per execution. Cache aggressively with Azure Cache for Redis to reduce database costs and improve latency. Use Azure CDN for static assets. For storage, implement lifecycle management policies to move infrequently accessed blobs to cool/archive tiers. Use reserved instances (1-year or 3-year) for baseline compute that's always running. Monitor costs with Azure Cost Management, set budgets and alerts. Consider spot instances for fault-tolerant batch processing. Use Application Insights sampling to control monitoring costs at scale. The key principle: separate baseline (reserved) from burst (auto-scale/consumption) to optimize the cost curve.",
                Difficulty: "Senior",
                Tags: ["cost-optimization", "scaling", "azure", "architecture"]),
        ],
        KeyConcepts:
        [
            "Cloud-native patterns (12-factor app)",
            "Managed identity and zero-trust",
            "Auto-scaling and scale-to-zero",
            "Message brokers vs event streaming",
            "Infrastructure as Code",
            "Cost optimization strategies",
            "Resilience patterns (retry, circuit breaker)",
            "Strangler fig migration",
        ],
        Resources:
        [
            new("Azure Architecture Center", "Documentation", "https://learn.microsoft.com/en-us/azure/architecture/"),
            new("Cloud Design Patterns", "Documentation", "https://learn.microsoft.com/en-us/azure/architecture/patterns/"),
            new("Microsoft.Extensions.Http.Resilience", "Documentation", "https://learn.microsoft.com/en-us/dotnet/core/resilience/"),
            new("Azure Well-Architected Framework", "Documentation", "https://learn.microsoft.com/en-us/azure/well-architected/"),
        ]);

    // ───────────────────────────────────────────────
    // Topic 10: Security
    // ───────────────────────────────────────────────
    private static TopicArea BuildSecurity() => new(
        Id: "security",
        Name: "Application Security & OWASP",
        Description: "OWASP Top 10, authentication/authorization patterns, secure coding in .NET, and common vulnerability prevention.",
        Questions:
        [
            new(
                Question: "Walk through the OWASP Top 10 (2021) and explain which ones are most relevant to .NET web applications.",
                ExpectedAnswer: "The OWASP Top 10 (2021): A01 Broken Access Control — most critical for .NET apps, ensure authorization checks on every endpoint (use [Authorize] attribute, policy-based authorization, resource-based checks). A02 Cryptographic Failures — use Data Protection API for encryption, never roll your own crypto, use HTTPS everywhere. A03 Injection — SQL injection mitigated by EF Core parameterized queries and Dapper parameters, but raw SQL concatenation is still a risk. XSS is mitigated by Razor's automatic HTML encoding but watch for @Html.Raw() and JavaScript contexts. A04 Insecure Design — threat modeling, proper domain validation. A05 Security Misconfiguration — don't expose detailed errors in production, disable directory browsing, use security headers (CSP, HSTS, X-Frame-Options). A06 Vulnerable Components — regularly update NuGet packages, use dotnet-outdated or Dependabot. A07 Authentication Failures — use ASP.NET Core Identity or external providers (OAuth/OIDC), enforce MFA, rate-limit login attempts. A08 Data Integrity Failures — verify NuGet package signatures, use SRI for CDN scripts. A09 Logging Failures — use structured logging but never log sensitive data (PII, tokens). A10 SSRF — validate and sanitize URLs when making outbound HTTP calls from user input.",
                Difficulty: "Senior",
                Tags: ["owasp", "web-security", "vulnerabilities"]),
            new(
                Question: "Explain OAuth 2.0 and OpenID Connect. How do they work together in an ASP.NET Core application?",
                ExpectedAnswer: "OAuth 2.0 is an authorization framework — it issues access tokens that grant limited access to resources on behalf of a user, without sharing credentials. OpenID Connect (OIDC) is an authentication layer built on top of OAuth 2.0 — it adds an ID token (JWT containing user identity claims) and a UserInfo endpoint. Together: OIDC handles 'who is this user?' and OAuth handles 'what can this user access?'. In ASP.NET Core, you configure OIDC authentication with builder.Services.AddAuthentication().AddOpenIdConnect(), specifying the identity provider (Azure AD, Auth0, Keycloak). The flow: user clicks login, browser redirects to the identity provider's authorize endpoint, user authenticates, provider redirects back with an authorization code, your app exchanges the code for tokens (ID token + access token + refresh token) at the token endpoint. The ID token is validated and used to create a ClaimsPrincipal. Access tokens are sent to APIs in the Authorization header. Use AddJwtBearer for API-only services that validate incoming access tokens. Use PKCE (Proof Key for Code Exchange) for all flows — it prevents authorization code interception attacks.",
                Difficulty: "Senior",
                Tags: ["oauth", "oidc", "authentication", "authorization"]),
            new(
                Question: "How do you prevent SQL injection in .NET applications? Are ORMs fully safe?",
                ExpectedAnswer: "SQL injection occurs when untrusted input is concatenated into SQL queries, allowing attackers to modify query logic. Prevention: always use parameterized queries. With EF Core, LINQ queries are automatically parameterized. With Dapper, use parameterized queries: connection.QueryAsync<T>(\"SELECT * FROM Users WHERE Id = @Id\", new { Id = userId }). With raw ADO.NET, use SqlParameter. ORMs are NOT fully safe: EF Core's FromSqlRaw and ExecuteSqlRaw accept raw SQL strings and will happily concatenate user input if you use string interpolation. Use FromSqlInterpolated or ExecuteSqlInterpolated instead — these look similar but properly parameterize the interpolated values. Also watch for dynamic ORDER BY or LIKE patterns constructed via string concatenation. Stored procedures add a layer of defense but are not immune if they use dynamic SQL internally (EXEC(@sql)). Additional defenses: principle of least privilege (database user should only have necessary permissions), input validation as defense-in-depth (not a primary defense), and WAF rules to catch common injection patterns.",
                Difficulty: "Mid",
                Tags: ["sql-injection", "orm", "parameterized-queries"]),
            new(
                Question: "What is Cross-Site Request Forgery (CSRF) and how does ASP.NET Core protect against it?",
                ExpectedAnswer: "CSRF tricks an authenticated user's browser into making unwanted requests to a web application. If a user is logged into a banking site and visits a malicious page, that page can submit a hidden form to the banking site — the browser automatically includes cookies, so the request looks legitimate. ASP.NET Core protects against CSRF with antiforgery tokens. For MVC/Razor Pages: forms generated with tag helpers automatically include a __RequestVerificationToken hidden field, and the [ValidateAntiForgeryToken] attribute (or AutoValidateAntiforgeryToken globally) verifies it on POST/PUT/DELETE. The system generates a pair: one token in a cookie and one in the form field. On submission, both must match. For APIs: if your API uses cookie authentication (not recommended), you need CSRF protection. APIs using Bearer token authentication (JWT in Authorization header) are inherently CSRF-proof because the attacker's page cannot add the Authorization header cross-origin. SameSite cookie attribute (Lax or Strict) provides additional defense by preventing cookies from being sent on cross-site requests. Best practice for SPAs: use token-based auth (JWT) with the token stored in memory (not localStorage), making CSRF impossible.",
                Difficulty: "Mid",
                Tags: ["csrf", "web-security", "antiforgery"]),
            new(
                Question: "How do you implement Content Security Policy (CSP) and what attacks does it mitigate?",
                ExpectedAnswer: "Content Security Policy is an HTTP response header that tells the browser which sources are allowed to load scripts, styles, images, fonts, frames, and other resources. It primarily mitigates XSS attacks by preventing inline scripts and restricting script sources. Implementation: add the Content-Security-Policy header via middleware. A strict policy: default-src 'self'; script-src 'self' 'nonce-{random}'; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; font-src 'self'; connect-src 'self' https://api.yourservice.com; frame-ancestors 'none'. The nonce approach generates a random nonce per request, adds it to allowed scripts and to the script tags — only scripts with the matching nonce execute. This blocks injected scripts. Start with Content-Security-Policy-Report-Only to test without breaking functionality — it reports violations without blocking. Implement a reporting endpoint to collect violation reports. Common issues: third-party scripts (analytics, CDN libraries) need explicit allowlisting, inline event handlers (onclick) are blocked by strict CSP — refactor to addEventListener. In ASP.NET Core, use the NWebsec library or custom middleware to manage CSP headers.",
                Difficulty: "Senior",
                Tags: ["csp", "xss", "security-headers"]),
            new(
                Question: "Explain the Data Protection API in ASP.NET Core and when to use it.",
                ExpectedAnswer: "The ASP.NET Core Data Protection API provides a managed cryptographic API for protecting data — encrypting and decrypting payloads with automatic key management. It replaces the old MachineKey from classic ASP.NET. Use cases: protecting cookies (authentication cookies are encrypted/signed with it by default), generating secure tokens (password reset, email confirmation), protecting sensitive data at rest. It handles key generation, rotation, and retirement automatically. Keys are stored in a key ring — by default in the file system for development, but in production you should configure persistent storage (Azure Blob Storage, Redis, database) and key protection (Azure Key Vault, DPAPI, X.509 certificates). When running multiple server instances behind a load balancer, all instances must share the same key ring and application name. Use IDataProtector for custom encryption: inject IDataProtectionProvider, create a protector with CreateProtector('purpose-string'), then call Protect/Unprotect. The purpose string isolates different protection contexts so a token created for one purpose cannot be unprotected with a different purpose. For time-limited tokens, use ITimeLimitedDataProtector.",
                Difficulty: "Senior",
                Tags: ["data-protection", "encryption", "asp-net-core"]),
            new(
                Question: "What are the security best practices for handling JWT tokens in a .NET application?",
                ExpectedAnswer: "Issuing: use asymmetric signing (RS256/ES256) so the API only needs the public key to validate — never distribute the signing key. Set short expiration times (5-15 minutes) and use refresh tokens for session continuity. Include only necessary claims — JWTs are base64-encoded (not encrypted), so anyone can read them. Never put sensitive data (passwords, PII) in JWT claims. Validation: always validate issuer, audience, expiration, and signature. Use AddJwtBearer with TokenValidationParameters configuring ValidIssuer, ValidAudience, and IssuerSigningKey. Storage in SPAs: store access tokens in memory (JavaScript variable), NOT in localStorage (accessible to XSS) or sessionStorage. Use short-lived access tokens with refresh token rotation via httpOnly secure cookies. Refresh tokens: store securely server-side or as httpOnly cookies, implement rotation (issue new refresh token on each use, invalidate the old one), and detect reuse (if an old refresh token is used, revoke all tokens for that user — it indicates theft). Revocation: since JWTs are stateless, you can't truly revoke them before expiration. For immediate revocation needs, maintain a deny-list checked on each request or use short expiration times.",
                Difficulty: "Senior",
                Tags: ["jwt", "authentication", "token-security"]),
        ],
        KeyConcepts:
        [
            "OWASP Top 10 (2021)",
            "OAuth 2.0 / OpenID Connect flows",
            "JWT security best practices",
            "CSRF and XSS prevention",
            "Content Security Policy",
            "Data Protection API",
            "SQL injection prevention",
            "Security headers (HSTS, CSP, X-Frame-Options)",
        ],
        Resources:
        [
            new("OWASP Top 10", "Documentation", "https://owasp.org/www-project-top-ten/"),
            new("ASP.NET Core Security", "Documentation", "https://learn.microsoft.com/en-us/aspnet/core/security/"),
            new("OAuth 2.0 Simplified", "Article", "https://aaronparecki.com/oauth-2-simplified/"),
            new("ASP.NET Core Data Protection", "Documentation", "https://learn.microsoft.com/en-us/aspnet/core/security/data-protection/"),
        ]);

    // ───────────────────────────────────────────────
    // Topic 11: Testing
    // ───────────────────────────────────────────────
    private static TopicArea BuildTesting() => new(
        Id: "testing",
        Name: "Testing Strategies & Practices",
        Description: "Unit testing, integration testing, TDD/BDD patterns, test architecture, and quality assurance approaches in .NET.",
        Questions:
        [
            new(
                Question: "What is the testing pyramid and how do you apply it in a .NET solution?",
                ExpectedAnswer: "The testing pyramid has three layers: unit tests at the base (fast, isolated, many), integration tests in the middle (slower, verify component interactions, fewer), and end-to-end/UI tests at the top (slowest, most brittle, fewest). In a .NET solution: Unit tests use xUnit/NUnit with Moq/NSubstitute for mocking dependencies. They test individual classes/methods in isolation — business logic, domain models, validators, calculators. Aim for hundreds to thousands of these. Integration tests use WebApplicationFactory<T> to spin up an in-memory test server, testing the full middleware pipeline, dependency injection, database interactions (using Testcontainers for real PostgreSQL/SQL Server instances). These verify that your controllers, services, repositories, and database work together correctly. Aim for dozens to hundreds. E2E tests use Playwright or Selenium to drive a real browser against the deployed application, testing critical user journeys. Keep these minimal (10-30) because they're slow and brittle. The anti-pattern is an 'ice cream cone' — many E2E tests, few unit tests — which leads to slow, flaky CI pipelines.",
                Difficulty: "Mid",
                Tags: ["testing-pyramid", "test-strategy", "xunit"]),
            new(
                Question: "How do you write integration tests for ASP.NET Core APIs using WebApplicationFactory?",
                ExpectedAnswer: "WebApplicationFactory<TEntryPoint> creates an in-memory test server hosting your application. Create a custom factory class that extends WebApplicationFactory<Program> and overrides ConfigureWebHost to replace production services with test versions — swap the real database for an in-memory database or Testcontainers instance, replace external HTTP clients with mocked handlers, and adjust configuration. In the test: use factory.CreateClient() to get an HttpClient that makes requests to the in-memory server. Write tests that make real HTTP requests: var response = await client.GetAsync('/api/products'); assert on status codes, response bodies, and headers. For authenticated endpoints, add a test authentication handler that bypasses real auth and sets claims. Use IClassFixture<CustomWebApplicationFactory> for shared factory across tests in a class, and implement IAsyncLifetime for per-test database seeding and cleanup. Key advantages: tests the full middleware pipeline (routing, model binding, validation, filters, serialization), catches configuration errors, and runs fast (no real network or web server).",
                Difficulty: "Senior",
                Tags: ["integration-testing", "webapplicationfactory", "asp-net-core"]),
            new(
                Question: "Explain the Arrange-Act-Assert pattern and how mocking frameworks like Moq or NSubstitute fit in.",
                ExpectedAnswer: "Arrange-Act-Assert (AAA) structures each test into three clear phases: Arrange sets up the test data, mock dependencies, and the system under test. Act invokes the method or action being tested — ideally a single line. Assert verifies the expected outcome — return values, state changes, or interactions with dependencies. Mocking frameworks fit into the Arrange phase. Moq: var mockRepo = new Mock<IUserRepository>(); mockRepo.Setup(r => r.GetByIdAsync(1)).ReturnsAsync(new User { Id = 1, Name = \"Alice\" }); var service = new UserService(mockRepo.Object). NSubstitute has a cleaner syntax: var repo = Substitute.For<IUserRepository>(); repo.GetByIdAsync(1).Returns(new User { Id = 1, Name = \"Alice\" }); var service = new UserService(repo). In the Assert phase, verify interactions: mockRepo.Verify(r => r.SaveAsync(It.IsAny<User>()), Times.Once) with Moq, or repo.Received(1).SaveAsync(Arg.Any<User>()) with NSubstitute. Best practice: prefer state-based assertions (check return values and state) over interaction-based assertions (verify method calls) — the latter couples tests to implementation details.",
                Difficulty: "Mid",
                Tags: ["aaa-pattern", "moq", "nsubstitute", "unit-testing"]),
            new(
                Question: "What are Testcontainers and why should you use them instead of in-memory databases for integration tests?",
                ExpectedAnswer: "Testcontainers is a library that manages Docker containers for integration testing. You spin up real database instances (PostgreSQL, SQL Server, MongoDB, Redis) in Docker containers that are automatically created before tests and destroyed after. Why use them instead of EF Core InMemory provider: the InMemory provider doesn't support transactions, doesn't enforce constraints (foreign keys, unique indexes), doesn't support raw SQL, and behaves differently from real databases in subtle ways (case sensitivity, NULL handling, query translation). Tests that pass with InMemory can fail in production. With Testcontainers, you test against the exact same database engine you use in production. Usage: install Testcontainers.PostgreSql NuGet package, create a PostgreSqlContainer in your test fixture's InitializeAsync, get the connection string via container.GetConnectionString(), and configure your DbContext to use it. The container starts in seconds and is isolated per test class. Combine with Respawn library to efficiently reset database state between tests (truncates all tables) instead of recreating the container for each test.",
                Difficulty: "Senior",
                Tags: ["testcontainers", "integration-testing", "docker"]),
            new(
                Question: "Describe TDD (Test-Driven Development) vs BDD (Behavior-Driven Development). How do they differ in practice?",
                ExpectedAnswer: "TDD follows the Red-Green-Refactor cycle: write a failing test first (Red), write the minimum code to make it pass (Green), then improve the code structure (Refactor). Tests are written from the developer's perspective, testing implementation details at the class/method level. It drives design by forcing you to think about the interface before the implementation and produces highly testable, loosely coupled code. BDD extends TDD by writing tests in business-readable language using Given-When-Then format. Tests describe behavior from the user's or stakeholder's perspective. In .NET, use SpecFlow (Gherkin syntax) or xBehave.net. Example: 'Given a registered user, When they submit valid login credentials, Then they should receive an access token.' BDD bridges the communication gap between developers and business stakeholders — the same specification serves as documentation and automated tests. In practice, many teams use TDD for unit tests (internal component behavior) and BDD for acceptance tests (user-facing features). The key difference is perspective: TDD tests code, BDD tests behavior. They're complementary, not mutually exclusive.",
                Difficulty: "Mid",
                Tags: ["tdd", "bdd", "specflow", "methodology"]),
            new(
                Question: "How do you test code that depends on external HTTP services? Describe different approaches.",
                ExpectedAnswer: "Several approaches from most to least isolated: 1) HttpMessageHandler mocking — create a mock handler that returns predetermined responses and inject it into HttpClient. This tests your code's handling of various responses without any network calls. 2) WireMock.Net — runs a local HTTP server that returns configured responses based on request matching rules. More realistic than handler mocking, supports recording real API responses for replay, and tests actual HTTP serialization. 3) Custom fake server — use WebApplicationFactory to host a minimal API that mimics the external service's contract. Good for complex interaction scenarios. 4) Contract testing with Pact — define a contract between consumer and provider, run consumer tests against a mock provider, then verify the provider satisfies the contract. Catches breaking API changes early. 5) Integration tests with the real service — only for final verification in a staging environment, never in CI due to flakiness and external dependency. Best practice: use handler mocking for unit tests, WireMock.Net for integration tests, and consider Pact for cross-team API contracts. Always test error scenarios: timeouts, 500s, rate limiting, malformed responses.",
                Difficulty: "Senior",
                Tags: ["http-testing", "wiremock", "mocking", "integration-testing"]),
            new(
                Question: "What is code coverage and what are its limitations? What coverage target should a team aim for?",
                ExpectedAnswer: "Code coverage measures the percentage of code lines, branches, or paths executed during tests. In .NET, use Coverlet (integrated with dotnet test via --collect:'XPlat Code Coverage') and ReportGenerator for HTML reports. Limitations: high coverage doesn't mean high quality — you can have 100% coverage with tests that assert nothing (no meaningful assertions). Coverage doesn't measure test quality, edge case handling, or correctness. It also encourages testing trivial code (getters, setters) to hit numbers rather than testing complex business logic. Coverage targets: 70-80% is a reasonable target for most projects. Focus coverage on business logic, domain models, and complex algorithms — not on infrastructure code, DTOs, or UI glue code. Use coverage as a guide to find untested code paths, not as a metric to maximize. Branch coverage is more valuable than line coverage — it reveals untested conditional paths. Some teams use coverage ratcheting: the coverage percentage must never decrease, enforced in CI, which prevents regression without mandating an arbitrary target.",
                Difficulty: "Mid",
                Tags: ["code-coverage", "coverlet", "quality-metrics"]),
        ],
        KeyConcepts:
        [
            "Testing pyramid (unit/integration/E2E)",
            "WebApplicationFactory for ASP.NET Core testing",
            "Testcontainers for real database testing",
            "Mocking with Moq and NSubstitute",
            "TDD and BDD methodology",
            "Code coverage analysis and limitations",
            "Contract testing with Pact",
            "Test isolation and determinism",
        ],
        Resources:
        [
            new("ASP.NET Core Integration Testing", "Documentation", "https://learn.microsoft.com/en-us/aspnet/core/test/integration-tests"),
            new("Testcontainers for .NET", "Documentation", "https://dotnet.testcontainers.org/"),
            new("xUnit Documentation", "Documentation", "https://xunit.net/docs/getting-started/netcore/cmdline"),
            new("Unit Testing Best Practices", "Documentation", "https://learn.microsoft.com/en-us/dotnet/core/testing/unit-testing-best-practices"),
        ]);

    // ───────────────────────────────────────────────
    // Topic 12: Frontend for Full-Stack
    // ───────────────────────────────────────────────
    private static TopicArea BuildFrontend() => new(
        Id: "frontend",
        Name: "Frontend for Full-Stack .NET Developers",
        Description: "Blazor, React/Angular integration with .NET APIs, TypeScript, SPA architecture, and modern frontend patterns.",
        Questions:
        [
            new(
                Question: "Compare Blazor Server, Blazor WebAssembly, and Blazor United (.NET 8+ render modes). When would you choose each?",
                ExpectedAnswer: "Blazor Server renders components on the server and sends UI diffs to the browser over a SignalR WebSocket connection. Advantages: fast initial load, full access to server resources, works on thin clients. Downsides: requires constant connection, latency on every interaction, server memory per user, doesn't work offline. Blazor WebAssembly runs .NET code directly in the browser via WebAssembly. Advantages: true client-side SPA, works offline, no server needed for UI rendering. Downsides: large initial download (framework DLLs), limited to browser sandbox capabilities, slower startup. Blazor United (Auto render mode in .NET 8+) combines both: initial render uses Server mode for fast first paint, then seamlessly transitions to WebAssembly once the runtime downloads in the background. You can also set render modes per-component: static SSR for content pages, Server for interactive forms, WebAssembly for client-heavy components. Choose Blazor Server for internal tools with reliable networks, WebAssembly for public-facing apps needing offline support, and Auto/United for the best of both worlds when you can handle the complexity.",
                Difficulty: "Senior",
                Tags: ["blazor", "spa", "rendering-modes"]),
            new(
                Question: "How do you integrate a React or Angular SPA frontend with an ASP.NET Core backend?",
                ExpectedAnswer: "Two main approaches: separate deployments or the SPA middleware pattern. Separate deployments (recommended for large teams): the SPA is its own project with its own build pipeline, deployed to a CDN or static hosting (Azure Static Web Apps, S3+CloudFront). The ASP.NET Core API is deployed separately. The SPA calls the API via CORS-configured HTTP endpoints. This gives frontend and backend teams independent deploy cycles. Configure CORS in ASP.NET Core with AddCors and UseCors, specifying allowed origins. SPA middleware pattern: UseStaticFiles serves the built SPA from wwwroot, with a fallback to index.html for client-side routing (so /products/123 serves index.html and React Router handles it). Use the SPA proxy in development for hot reload. Authentication: use OAuth 2.0/OIDC — the SPA gets tokens from the identity provider and sends them as Bearer tokens to the API. For BFF (Backend for Frontend) pattern, the ASP.NET Core app handles authentication, stores tokens server-side, and proxies API calls — the SPA never touches tokens directly, which is more secure. YARP can be used as the BFF reverse proxy.",
                Difficulty: "Senior",
                Tags: ["react", "angular", "spa", "api-integration"]),
            new(
                Question: "What is TypeScript and why should a .NET backend developer learn it?",
                ExpectedAnswer: "TypeScript is a statically typed superset of JavaScript developed by Microsoft. It compiles to plain JavaScript and adds type annotations, interfaces, enums, generics, and advanced type system features (union types, mapped types, conditional types). For .NET developers, TypeScript feels familiar — it shares many concepts with C# since Anders Hejlsberg designed both languages. Interfaces, generics, async/await, and class-based OOP work similarly. Why learn it: if you're full-stack, you'll write frontend code, and TypeScript catches entire categories of bugs at compile time that JavaScript would only reveal at runtime. It provides IntelliSense, refactoring support, and documentation through types. It's the standard for all major frameworks (React, Angular, Vue). You can share type definitions between .NET and TypeScript — tools like NSwag generate TypeScript client code from your OpenAPI/Swagger specification, ensuring your API contract is type-safe end-to-end. For .NET developers specifically: learn TypeScript's structural typing (vs C#'s nominal typing), union types (no direct C# equivalent), and the module system.",
                Difficulty: "Mid",
                Tags: ["typescript", "full-stack", "javascript"]),
            new(
                Question: "Explain SignalR and real-time communication patterns in .NET. When would you use it?",
                ExpectedAnswer: "SignalR is a library for adding real-time bidirectional communication to .NET applications. It abstracts over multiple transport protocols — WebSockets (preferred), Server-Sent Events, and long polling — automatically selecting the best available transport. The programming model uses Hubs: server-side Hub classes define methods that clients can invoke, and the hub can push messages to connected clients (individual, groups, or all). Use cases: live dashboards with real-time metrics, chat applications, collaborative editing, real-time notifications, live auction bidding, progress updates for long-running operations, multiplayer games. In ASP.NET Core, add SignalR with builder.Services.AddSignalR() and map hubs with app.MapHub<ChatHub>('/chatHub'). For scaling across multiple servers, use a backplane — Azure SignalR Service (managed, scales to millions of connections), Redis backplane, or SQL Server backplane. Clients available for JavaScript, .NET, Java, and Python. For simple server-to-client streaming (no bidirectional needed), consider Server-Sent Events directly. For high-throughput binary data, consider raw WebSockets.",
                Difficulty: "Mid",
                Tags: ["signalr", "real-time", "websockets"]),
            new(
                Question: "What are Web Components and how can they be used with Blazor or ASP.NET Core?",
                ExpectedAnswer: "Web Components are a set of browser-native APIs for creating reusable custom HTML elements: Custom Elements (define new HTML tags), Shadow DOM (encapsulated styles and markup), HTML Templates (declarative template fragments), and ES Modules (standard import mechanism). They work with any framework or no framework. In Blazor, you can use Web Components by including their JavaScript files and using them as regular HTML elements in Razor components — Blazor's event system interoperates via JSInterop. You can also expose Blazor components AS Web Components using the RegisterCustomElement API introduced in .NET 7, allowing Blazor components to be used in React, Angular, or plain HTML pages. In ASP.NET Core without Blazor, Web Components work as any HTML — include the component's script, use the custom element in your Razor Pages or MVC views. This is valuable for design systems: build your component library as Web Components once, use them everywhere regardless of the server framework. Libraries like Lit make building Web Components ergonomic with TypeScript support.",
                Difficulty: "Senior",
                Tags: ["web-components", "blazor", "interoperability"]),
            new(
                Question: "How do you handle API versioning in a .NET backend to avoid breaking frontend clients?",
                ExpectedAnswer: "ASP.NET Core supports multiple API versioning strategies via the Asp.Versioning.Http and Asp.Versioning.Mvc packages. URL path versioning (/api/v1/products, /api/v2/products) — most visible and easiest for clients to understand, but changes the URL contract. Query string versioning (/api/products?api-version=2.0) — doesn't change the URL path but is easy to forget. Header versioning (X-Api-Version: 2.0) — cleanest URLs but less discoverable. Media type versioning (Accept: application/json;v=2) — most RESTful but complex to implement. Implementation: add builder.Services.AddApiVersioning(options => { options.DefaultApiVersion = new ApiVersion(1,0); options.ReportApiVersions = true; options.AssumeDefaultVersionWhenUnspecified = true; }). Decorate controllers with [ApiVersion(\"1.0\")] and [ApiVersion(\"2.0\")]. Best practices: never remove fields from responses (only add), use response envelope with version field, maintain a deprecation policy (6+ months notice), generate versioned OpenAPI specs, and auto-generate versioned TypeScript clients. The most practical approach for most teams is URL path versioning — it's explicit and works with all tooling.",
                Difficulty: "Senior",
                Tags: ["api-versioning", "backward-compatibility", "rest"]),
        ],
        KeyConcepts:
        [
            "Blazor render modes (Server/WASM/Auto)",
            "SPA integration patterns (CORS, BFF)",
            "TypeScript for .NET developers",
            "SignalR real-time communication",
            "API versioning strategies",
            "Web Components interoperability",
        ],
        Resources:
        [
            new("Blazor Documentation", "Documentation", "https://learn.microsoft.com/en-us/aspnet/core/blazor/"),
            new("SignalR Documentation", "Documentation", "https://learn.microsoft.com/en-us/aspnet/core/signalr/"),
            new("TypeScript Handbook", "Documentation", "https://www.typescriptlang.org/docs/handbook/"),
            new("API Versioning in ASP.NET Core", "Documentation", "https://learn.microsoft.com/en-us/aspnet/core/web-api/advanced/conventions"),
        ]);

    // ───────────────────────────────────────────────
    // Topic 13: DevOps & Production
    // ───────────────────────────────────────────────
    private static TopicArea BuildDevOps() => new(
        Id: "devops",
        Name: "DevOps, CI/CD & Production Operations",
        Description: "Docker, Kubernetes, CI/CD pipelines, observability, logging, monitoring, and troubleshooting .NET applications in production.",
        Questions:
        [
            new(
                Question: "How do you containerize a .NET application with Docker? Walk through a production-ready Dockerfile.",
                ExpectedAnswer: "A production Dockerfile uses multi-stage builds to minimize image size and attack surface. Stage 1 (build): FROM mcr.microsoft.com/dotnet/sdk:10.0 AS build — restore NuGet packages first (COPY *.csproj and dotnet restore) for layer caching, then COPY source and dotnet publish -c Release -o /app. Stage 2 (runtime): FROM mcr.microsoft.com/dotnet/aspnet:10.0 AS runtime — this image is much smaller (no SDK, no compiler). COPY --from=build /app . and set ENTRYPOINT. Production hardening: run as non-root user (RUN adduser --disabled-password appuser, USER appuser), set ASPNETCORE_ENVIRONMENT=Production, use .dockerignore to exclude bin/obj/tests, set health check (HEALTHCHECK CMD curl --fail http://localhost:8080/healthz), and use specific image tags (not :latest). For even smaller images, use chiseled images (mcr.microsoft.com/dotnet/aspnet:10.0-noble-chiseled) — Ubuntu-based with everything stripped except what .NET needs, non-root by default, no shell, no package manager. Enable globalization invariant mode if you don't need culture-specific formatting. Use BuildKit with --mount=type=cache for NuGet cache to speed up rebuilds.",
                Difficulty: "Mid",
                Tags: ["docker", "containerization", "multi-stage-build"]),
            new(
                Question: "Describe a CI/CD pipeline for a .NET application. What stages should it have?",
                ExpectedAnswer: "A robust CI/CD pipeline for .NET: 1) Trigger — on push to main, pull request, or tag creation. 2) Restore — dotnet restore with NuGet cache (actions/cache or pipeline caching). 3) Build — dotnet build --no-restore in Release configuration. 4) Test — dotnet test with code coverage collection (--collect:'XPlat Code Coverage'). Run unit tests and integration tests (using Testcontainers with Docker-in-Docker or service containers). Fail the pipeline on any test failure. 5) Code Quality — run dotnet format --verify-no-changes for formatting, analyzers (Roslyn analyzers built into the project), and optionally SonarQube/SonarCloud for deeper analysis. 6) Security Scan — run dotnet list package --vulnerable to check for known vulnerabilities in NuGet packages. Run container scanning if building Docker images. 7) Publish — dotnet publish -c Release, build Docker image, push to container registry (ACR, ECR, GHCR). Tag with Git SHA and semantic version. 8) Deploy to Staging — deploy to staging environment, run smoke tests and E2E tests. 9) Deploy to Production — manual approval gate, then rolling deployment or blue-green deployment. Post-deploy: run health check verification. Tools: GitHub Actions, Azure DevOps Pipelines, or GitLab CI.",
                Difficulty: "Senior",
                Tags: ["ci-cd", "github-actions", "azure-devops", "pipeline"]),
            new(
                Question: "What is observability and how do you implement the three pillars (logs, metrics, traces) in .NET?",
                ExpectedAnswer: "Observability is the ability to understand a system's internal state from its external outputs. Three pillars: Logs — structured events recording what happened. Use Serilog or Microsoft.Extensions.Logging with structured logging (logger.LogInformation('Order {OrderId} processed in {Duration}ms', orderId, elapsed)). Send to centralized log aggregation (Seq, Elasticsearch/Kibana, Azure Monitor, Datadog). Metrics — numerical measurements aggregated over time. Use System.Diagnostics.Metrics (built-in .NET 8+) or Prometheus with prometheus-net. Track request duration histograms, error rates, queue depths, business metrics (orders/minute). Expose /metrics endpoint for Prometheus scraping or push to Azure Monitor/Datadog. Traces — distributed request tracking across services. Use System.Diagnostics.Activity and OpenTelemetry. AddOpenTelemetry() in DI configures automatic instrumentation for ASP.NET Core, HttpClient, EF Core, and gRPC. Traces propagate context (trace ID, span ID) across service boundaries via W3C TraceContext headers. Export to Jaeger, Zipkin, Azure Monitor, or any OTLP-compatible backend. Together: when an alert fires on a metric (high error rate), you look at traces to find which service and operation is failing, then look at logs for that trace ID to get the error details.",
                Difficulty: "Senior",
                Tags: ["observability", "opentelemetry", "logging", "monitoring"]),
            new(
                Question: "How do you troubleshoot a .NET application that is running slowly in production?",
                ExpectedAnswer: "Systematic approach: 1) Check metrics first — is it CPU-bound, memory-bound, I/O-bound, or thread pool starvation? Check CPU utilization, memory usage, GC metrics (% time in GC, gen2 collection rate), thread pool queue length, and active request count. 2) If CPU-bound: capture a CPU profile using dotnet-trace or dotnet-monitor. Analyze the trace in PerfView or Speedscope to find hot methods. Common causes: inefficient algorithms, excessive string concatenation, regex without compiled option, unnecessary serialization. 3) If memory-bound: capture a memory dump with dotnet-dump, analyze with dotnet-gcdump or Visual Studio. Look for large objects on LOH, excessive gen2 collections, memory leaks (objects rooted by static fields, event handlers not unsubscribed, IDisposable not disposed). 4) If I/O-bound: check database query performance (slow queries, missing indexes, N+1 problems), external HTTP call latency (add timeout and circuit breaker), and check if async/await is used correctly (no sync-over-async blocking threads). 5) Thread pool starvation: check ThreadPool.PendingWorkItemCount and ThreadPool.ThreadCount. Caused by blocking calls (.Result, .Wait(), Thread.Sleep) on thread pool threads. Fix by making code fully async. Use dotnet-counters for real-time counters and dotnet-monitor for production-safe diagnostics.",
                Difficulty: "Senior",
                Tags: ["troubleshooting", "profiling", "performance", "diagnostics"]),
            new(
                Question: "Explain health checks in ASP.NET Core and how they integrate with container orchestrators.",
                ExpectedAnswer: "ASP.NET Core health checks provide endpoints that container orchestrators (Kubernetes, Docker Compose, Azure Container Apps) use to determine if the application is healthy. Configure with builder.Services.AddHealthChecks() and add checks for dependencies: .AddSqlServer(connectionString) for database connectivity, .AddRedis(redisConnection), .AddUrlGroup(new Uri('https://external-api.com')) for external services. Map endpoints: app.MapHealthChecks('/healthz') for liveness (is the process alive?) and app.MapHealthChecks('/ready', new HealthCheckOptions { Predicate = check => check.Tags.Contains('ready') }) for readiness (is the app ready to receive traffic?). In Kubernetes: livenessProbe calls /healthz — if it fails, Kubernetes restarts the pod. readinessProbe calls /ready — if it fails, Kubernetes removes the pod from the Service load balancer (no traffic) but doesn't restart it. startupProbe runs only during startup for slow-starting apps. Health checks return Healthy, Degraded, or Unhealthy status with optional details. Use AspNetCore.HealthChecks.UI for a visual dashboard. Tag-based filtering lets you separate critical checks (database) from optional checks (cache) so a Redis outage doesn't restart your pods.",
                Difficulty: "Mid",
                Tags: ["health-checks", "kubernetes", "readiness", "liveness"]),
            new(
                Question: "What are the key considerations for deploying .NET applications to Kubernetes?",
                ExpectedAnswer: "Resource management: set CPU and memory requests (guaranteed resources) and limits (maximum). .NET has server GC by default which is memory-hungry — for containers, configure DOTNET_GCHeapCount and use container-aware settings (enabled by default in .NET 8+). Graceful shutdown: handle SIGTERM by using IHostApplicationLifetime.ApplicationStopping to drain in-flight requests. Set terminationGracePeriodSeconds higher than your drain timeout. Use Kubernetes preStop hooks for delay before SIGTERM. Scaling: use Horizontal Pod Autoscaler (HPA) based on CPU, memory, or custom metrics (request rate via KEDA). Set appropriate minReplicas and maxReplicas. For event-driven scaling, use KEDA with Azure Service Bus, RabbitMQ, or Kafka triggers. Networking: use Kubernetes Services for service discovery, Ingress (NGINX, Traefik) or Gateway API for external access. Enable mTLS between services with a service mesh (Istio, Linkerd) or Dapr. Configuration: use ConfigMaps for non-sensitive config and Secrets (encrypted with External Secrets Operator from Key Vault) for credentials. Mount as environment variables or files. Rolling updates: set maxSurge and maxUnavailable, use readiness probes to prevent traffic to unready pods during deployment.",
                Difficulty: "Senior",
                Tags: ["kubernetes", "deployment", "scaling", "containers"]),
            new(
                Question: "How do you implement feature flags in a .NET application?",
                ExpectedAnswer: "Feature flags (feature toggles) decouple deployment from release — deploy code behind flags, enable features gradually. In .NET, use Microsoft.FeatureManagement: add builder.Services.AddFeatureManagement() which reads from IConfiguration (appsettings.json, Azure App Configuration). Define flags in config and check in code: if (await featureManager.IsEnabledAsync('NewCheckoutFlow')). Use [FeatureGate('FeatureName')] attribute on controllers/actions to gate entire endpoints. Advanced patterns with Azure App Configuration: percentage rollout (enable for 10% of users, then 50%, then 100%), targeting (enable for specific users/groups first — beta testers, internal employees), time window (enable between specific dates for seasonal features). Integrate with filters: built-in TimeWindowFilter, PercentageFilter, TargetingFilter, or create custom IFeatureFilter. For A/B testing, combine with application insights to measure feature impact. Key practices: flags should be short-lived — remove the flag code once a feature is fully launched. Long-lived operational flags (kill switches, maintenance mode) are the exception. Use a naming convention and maintain a flag registry to prevent flag proliferation.",
                Difficulty: "Mid",
                Tags: ["feature-flags", "feature-management", "deployment"]),
        ],
        KeyConcepts:
        [
            "Docker multi-stage builds and chiseled images",
            "CI/CD pipeline design",
            "OpenTelemetry and observability",
            "Production troubleshooting with dotnet tools",
            "Kubernetes deployment patterns",
            "Health checks (liveness/readiness)",
            "Feature flags and progressive delivery",
            "Graceful shutdown and drain",
        ],
        Resources:
        [
            new("Docker for .NET", "Documentation", "https://learn.microsoft.com/en-us/dotnet/core/docker/introduction"),
            new("OpenTelemetry .NET", "Documentation", "https://opentelemetry.io/docs/languages/dotnet/"),
            new("ASP.NET Core Health Checks", "Documentation", "https://learn.microsoft.com/en-us/aspnet/core/host-and-deploy/health-checks"),
            new("Feature Management in .NET", "Documentation", "https://learn.microsoft.com/en-us/azure/azure-app-configuration/use-feature-flags-dotnet-core"),
        ]);

    // ───────────────────────────────────────────────
    // Topic 14: Behavioral & Leadership
    // ───────────────────────────────────────────────
    private static TopicArea BuildBehavioral() => new(
        Id: "behavioral",
        Name: "Behavioral & Leadership",
        Description: "STAR method responses, leadership scenarios, conflict resolution, mentoring, and senior engineer soft skills for interviews.",
        Questions:
        [
            new(
                Question: "Tell me about a time you disagreed with a technical decision. How did you handle it?",
                ExpectedAnswer: "Use the STAR method: Situation — describe the context (team, project, timeline). Task — explain the decision point and why you disagreed (e.g., the team chose a microservices approach for a small project, but you believed a modular monolith was more appropriate given the team size and timeline). Action — describe how you handled it constructively: you gathered data (performance benchmarks, complexity estimates, team experience), presented your case clearly with pros and cons of both approaches, listened to the other side's reasoning, and proposed a compromise or accepted the team's decision gracefully. Result — what happened: either your approach was adopted and the outcome was positive, or you committed to the team's decision and contributed to making it succeed, and what you learned from the experience. Key behaviors to demonstrate: data-driven argumentation not emotional, 'disagree and commit' maturity, willingness to be wrong, and focus on team success over being right.",
                Difficulty: "Senior",
                Tags: ["disagreement", "collaboration", "decision-making"]),
            new(
                Question: "How do you mentor junior developers? Give a specific example.",
                ExpectedAnswer: "Effective mentoring involves structured and unstructured approaches. Structured: conduct regular 1-on-1s focused on growth goals, assign progressively challenging tasks (start with well-defined bug fixes, then small features, then design tasks), pair program on complex problems (let them drive, you navigate), and do thorough but kind code reviews that explain the 'why' not just the 'what'. Unstructured: be approachable for questions (create psychological safety so they're not afraid to ask 'dumb' questions), share your thought process out loud when debugging together, recommend specific learning resources based on their gaps. Example using STAR: 'I noticed a junior dev was struggling with async patterns and writing blocking code in ASP.NET Core. I set up a weekly 30-minute session where we refactored one service class each week to use proper async/await, explaining the state machine, thread pool implications, and common pitfalls. After 6 weeks, they were writing correct async code independently and even caught an async anti-pattern in a PR review.' Demonstrate that you invest in the team's growth, not just your own output.",
                Difficulty: "Senior",
                Tags: ["mentoring", "leadership", "growth"]),
            new(
                Question: "Describe a production incident you handled. What was your approach?",
                ExpectedAnswer: "STAR method: Situation — describe the severity and impact (e.g., 'Production API was returning 500 errors for 30% of requests during peak traffic, affecting thousands of users'). Task — your role in the incident (incident commander, lead debugger, or contributor). Action — systematic approach: 1) Triage: check monitoring dashboards, identify affected services, assess blast radius. 2) Communicate: update status page, notify stakeholders, establish a war room/Slack channel. 3) Mitigate: apply immediate mitigation (rollback deployment, scale up instances, enable circuit breaker, redirect traffic). Mitigation first, root cause second. 4) Diagnose: examine logs for the timeframe, check recent deployments, analyze traces, review metrics for anomalies. 5) Fix: apply the fix, verify in staging if possible, deploy with monitoring. Result — describe the outcome, timeline, and most importantly the post-mortem: blameless retrospective, root cause analysis (e.g., a database connection pool exhaustion caused by a missing async disposal), and preventive measures implemented (connection pool monitoring alert, integration test for disposal, runbook for similar incidents). Key traits: stay calm under pressure, communicate clearly, prioritize mitigation over blame.",
                Difficulty: "Senior",
                Tags: ["incident-response", "production", "communication"]),
            new(
                Question: "How do you approach estimating and planning a large technical project?",
                ExpectedAnswer: "Break the project into deliverable phases with clear milestones — don't try to plan everything upfront with perfect accuracy. Start with a design document or RFC that describes the problem, proposed solution, alternatives considered, and technical risks. Get buy-in from stakeholders and other senior engineers before coding. For estimation: break work into tasks no larger than 2-3 days. Use three-point estimation (optimistic, most likely, pessimistic) for risky items. Identify unknowns early and time-box spikes to reduce them (e.g., 'spend 2 days prototyping the new caching layer to validate the approach'). Add buffer for integration, testing, and unexpected issues — a common rule of thumb is to multiply estimates by 1.5-2x. For planning: use iterative delivery — define an MVP that delivers value earliest, then iterate. Identify dependencies and critical path. Communicate progress honestly — if you're falling behind, raise it early with data ('We're 3 days behind on the database migration, here are options: reduce scope of X, extend timeline, or add help'). Never pad estimates silently — be transparent about uncertainty. Track velocity over sprints to improve future estimation accuracy.",
                Difficulty: "Senior",
                Tags: ["estimation", "planning", "project-management"]),
            new(
                Question: "How do you handle a situation where you inherit a legacy codebase with no tests?",
                ExpectedAnswer: "First, resist the urge to rewrite everything — the existing code works and encodes business rules you might not fully understand. Approach: 1) Understand: read the code, talk to people who wrote it, document the system's behavior and architecture. Use logging and observability to understand runtime behavior. 2) Stabilize: add characterization tests (tests that document current behavior, not desired behavior) around the areas you'll be changing. Michael Feathers' 'Working Effectively with Legacy Code' calls these 'golden master' tests. Use integration tests initially since the code likely isn't designed for unit testing. 3) Establish guardrails: set up CI/CD if it doesn't exist, add a linter, enable basic monitoring and alerting. 4) Improve incrementally: when you need to change a section, first add tests around it, then refactor to improve testability (introduce interfaces, break dependencies), then make your change. This is the 'Boy Scout Rule' — leave the code better than you found it, but only where you're working. 5) Never mix refactoring with feature changes in the same PR — reviewers can't distinguish between behavior changes and structural improvements. Over time, the tested and improved area grows naturally around the most-changed code.",
                Difficulty: "Senior",
                Tags: ["legacy-code", "refactoring", "technical-debt"]),
            new(
                Question: "Tell me about a time you had to make a trade-off between code quality and meeting a deadline.",
                ExpectedAnswer: "STAR method: Situation — describe the business context that created the time pressure (e.g., 'A critical feature needed to ship for a client demo in 2 weeks, but doing it properly with full test coverage and clean architecture would take 4 weeks'). Task — your responsibility in making the trade-off decision. Action — describe how you made an informed decision: identified what could be deferred vs what was essential, documented the technical debt explicitly (created tickets with clear descriptions of what needs to be done), implemented with pragmatic shortcuts that were safe (e.g., used a simpler architecture but kept the code clean, skipped performance optimization but not correctness, wrote integration tests instead of comprehensive unit tests). Got buy-in from the team and product manager on what you were deferring. Result — the feature shipped on time, the demo succeeded, and you came back to address the technical debt in the next sprint as planned. The key lessons: technical debt is not inherently bad — it's a deliberate choice with understood trade-offs, like financial debt. The danger is untracked technical debt. Always document it, always have a plan to repay it, and never compromise on security or data integrity.",
                Difficulty: "Senior",
                Tags: ["trade-offs", "technical-debt", "deadlines", "pragmatism"]),
            new(
                Question: "How do you approach code reviews? What do you look for?",
                ExpectedAnswer: "Code review serves three purposes: catching bugs, maintaining quality standards, and knowledge sharing. What I look for, in priority order: 1) Correctness — does the code do what it's supposed to? Are edge cases handled? Are there race conditions or off-by-one errors? 2) Security — are there injection vulnerabilities, exposed secrets, missing authorization checks? 3) Design — does the approach make sense? Is it consistent with the codebase's patterns? Are the abstractions at the right level? 4) Maintainability — will someone (including the author) understand this in 6 months? Are names descriptive? Is the logic clear? 5) Testing — are the tests testing the right things? Are they resilient to refactoring? 6) Performance — only for hot paths or obvious performance issues. How I give feedback: be specific ('This could cause a null reference on line 42 when userId is empty' not 'This looks wrong'), explain why ('Using string concatenation in a loop creates O(n^2) allocations, consider StringBuilder'), distinguish between blocking comments and suggestions ('nit:' prefix for style preferences), and praise good solutions. Never make it personal — review the code, not the person.",
                Difficulty: "Mid",
                Tags: ["code-review", "collaboration", "quality"]),
            new(
                Question: "How do you stay up to date with technology as a senior .NET developer?",
                ExpectedAnswer: "Structured approach across multiple channels: Regular reading — follow the .NET blog (official announcements), read release notes for every .NET release, subscribe to curated newsletters (ASP.NET Community Standup, .NET Weekly, C# Digest). Follow key people on social media — David Fowler, Stephen Toub, Nick Chapsas, Andrew Lock, Steve Sanderson. Practice — build side projects to try new features (e.g., built a hobby project with .NET 8 to learn Native AOT and minimal APIs). Contribute to open source or read the source code of libraries you use. Community — attend or watch conference talks (NDC, .NET Conf, JetBrains Day), participate in local meetups or online communities (r/dotnet, Discord servers). Depth vs breadth — go deep on technologies relevant to your current work, maintain awareness of broader trends (AI/ML, cloud-native, WASM). Teaching — writing blog posts or mentoring forces you to understand topics deeply. Time management — dedicate specific time (e.g., Friday afternoons for learning), don't try to learn everything — focus on what aligns with your career goals and the market demand in your target companies. The key insight: you don't need to master every new technology, but you need to be able to evaluate when new approaches are worth adopting.",
                Difficulty: "Mid",
                Tags: ["learning", "career-growth", "technology-radar"]),
        ],
        KeyConcepts:
        [
            "STAR method (Situation-Task-Action-Result)",
            "Disagree and commit",
            "Blameless post-mortems",
            "Technical debt management",
            "Mentoring and knowledge sharing",
            "Estimation and planning",
            "Code review best practices",
            "Legacy code modernization",
        ],
        Resources:
        [
            new("Working Effectively with Legacy Code by Michael Feathers", "Book", "https://www.oreilly.com/library/view/working-effectively-with/0131177052/"),
            new("The Staff Engineer's Path by Tanya Reilly", "Book", "https://www.oreilly.com/library/view/the-staff-engineers/9781098118723/"),
            new("Behavioral Interview Preparation", "Article", "https://www.techinterviewhandbook.org/behavioral-interview/"),
            new("The Manager's Path by Camille Fournier", "Book", "https://www.oreilly.com/library/view/the-managers-path/9781491973882/"),
        ]);
}
